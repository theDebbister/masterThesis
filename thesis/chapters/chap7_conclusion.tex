\newchap{Conclusion}
\label{chap:6_conclusion}
As a first step of this thesis I created an overview of what phonetic data is available in which languages. The overwhelming majority of available data is word lists. But in general, there is quite a lot of data that I could use for my experiments. The WikiPron data keeps growing and even if it is not cleaned, the models were able to produce good results for quite many languages. One thing I noticed about the data is that phonetic text (given it is written in \ac{ipalpha}) is more complex and has more characters than most alphabets. The case for Zulu also showed that the tones (which are represented as diacritics in Zulu) add to that complexity. The comparison of the \ac{nws} transcriptions and the WikiPron test sets showed that phonetic transcriptions are not as standardized as typical writing systems. It might be worth looking into why exactly the models performance was so bad on the \ac{nws} short stories and how the transcription convention has an influence on the models' performance.

As \ac{g2p} conversion is a well-known \ac{s2s} task, there exist a lot of models and architectures that can be used. My experiments with the feature models showed that it is possible to manipulate the input and the model does still perform well. All in all, there were some good results produced for a large collection of many languages. Some of my models outperformed existing state-of-the-art models, for some there was no comparison available. It would certainly be good to train more \ac{g2p} models on more languages such that my results can be compared. 

An important lesson I learnt from this thesis is the overwhelming complexity of computational representations of different scripts and the \ac{ipalpha}. While I did not read a lot about this topic in present research, it was very important for my multilingual processing. Multilingualism in general presents a huge challenge. Starting from transcription conventions that are not always cross-linguistically applicable to machine learning models that are very carefully designed to work for one language only, it seems that many steps of creating \ac{g2p} models are not made for working with a huge multilingual dataset. Research about linguistic diversity and language comparison in \ac{nlp} datasets is certainly a good start to pave the way for more work with multilingual corpora.

\section*{Future work}
Non surprisingly, apart from many exciting things I \textit{could} do, there are many others that would have gone beyond the scope of this thesis. I would like to list a few entry points on where future research could start. 

\begin{itemize}
\item \textbf{Data preprocessing}: \cite{Ashby-Bartley.2021} cleaned broad transcriptions for Bulgarian and replaced all allophones by their standard phoneme. I also detected allophones in some broad transcriptions of some language. Replacing allophones by their respective phoneme could further improve model quality by having consistent broad transcriptions. Also, my results showed that cleaning the data did not always improve the models' performance. It might give interesting insights to look more into what preprocessing steps are actually necessary for which languages.
\item \textbf{Phonetic features}: Although my feature models did not outperform the other models on many languages, they did not perform a lot worse. In this thesis, I tried to add the features by manipulating the input to the model. It might also be possible to chose a different model architecture such that the features can be passed more explicitly to the model. I am sure that it is worth digging deeper into phonetic features and how we can use them to train better \ac{g2p} models.
\item \textbf{Low resource languages}: There are a lot of languages where only very little data is available. I did not deal with those in this thesis but it is technically possible to use transfer learning or other techniques to make use of those languages. Low resource languages still are a challenge.
\end{itemize}
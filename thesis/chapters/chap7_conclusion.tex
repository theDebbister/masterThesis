\newchap{Conclusion}
\label{chap:6_conclusion}
In the beginning I listed a few questions that I would like to answer while working on this thesis. While I cannot answer all of them in full, there are a few interesting insights.

\begin{enumerate}
\item What types of phonetic data is available and how can it be used?
\item Which computational models can be used to create phonetic transcriptions?
\item How can we use phonetic features to create phonetic transcriptions?
\item Is there any significant difference in comparing spoken or written languages?
\item Does written text represent language well enough to justify text-based research only?
\end{enumerate}

Concerning question one, the overwhelming majority of available data is word lists. But in general, there is quite a lot of data that I could use for my experiments. The WikiPron data keeps growing and even if it is not preprocessed a lot the models are able to produce good results. As \ac{g2p} conversion is a well-known \ac{s2s} task, there exist a lot of models and architectures that can be used. The experiments with the feature models showed that it is possible to manipulate the input and the model do still perform well. The last two questions are still open. However, one thing I did notice is that phonetic text (given it is written in \ac{ipalpha}) is more complex and has more characters than most alphabets. The case for Zulu also showed that the tones (which are represented as diacritics in Zulu) add to that complexity. The comparison of the \ac{nws} transcriptions and the WikiPron test sets showed that phonetic transcriptions are not as standardized as typical writing systems. 

All in all, there were some good results produced for a large collection of many languages. Some outperformed existing state-of-the-art models, for some there was no comparison available. 

An important lesson to be learnt from this thesis is the computational representation of different scripts and the \ac{ipalpha}. While I did not read a lot about this topic in present research, it was very important for my multilingual processing.

\section{Future work}
Non surprisingly, apart from many exciting things I \textit{could} do, there are many others that would have gone beyond the scope of this thesis. I would like to list a few entry points on where further research could start. 

\begin{itemize}
\item \textbf{Data preprocessing}: \cite{Ashby-Bartley.2021} cleaned broad transcriptions for Bulgarian and replaced allophones by their standard phoneme. This could further improve model quality by having consistent broad transcriptions.
\item \textbf{Phonetic features}: Although my models did not outperform the other models on many language, they did not perform a lot worse. In this thesis, I tried to add the features by manipulating the input to the model. It might also be possible to chose a different model architecture such that the features can be passed more explicitly to the model. I am sure that it is worth digging deeper into phonetic features and how we can use them.
\item \textbf{Low resource languages}: There are a lot of languages where only very little data is available. I did not deal with those in this thesis but it is technically possible to use transfer learning or other techniques to make use of those languages. Low resource languages still are a challenge.
\end{itemize}
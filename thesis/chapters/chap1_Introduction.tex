
\newchap{Introduction}
\label{chap:1_intro}

With the advent of technologies that can process huge amounts of data, many linguistic tasks that were originally very tiresome and expensive to do, can now be accomplished much faster. Well known examples for this branch called \ac{nlp} are machine translation or search engines. A lot of available tools and consequently research done in this area is concerned with written text. For many scenarios like machine translation, large corpora of written text in many languages are collected that are used to train computational models that solve those tasks. Following, there is an ever-growing set of models that are trained on written text. Often the goal is to reach or outperform human solutions to those various tasks. The corpora that are gathered for tasks like machine translation serve as an example for how the outcome of the task in question should look like. For example, a corpus for machine translation contains a text in one language with its corresponding translation into an other language. It is therefore necessary that this training data represents the language and the task well enough for a machine to reach the required performance. From a linguistic point of view, the questions comes up if focusing on written language only can ever represent human language adequately. Most of communication and daily language use happens through speaking. This is a first potential limitation of many (written) language technologies. It is not easy to find out what characteristics of a language can be observed in written representations. Although many \ac{nlp} tasks use written language corpora exclusively, there are technologies like \ac{asr} or \ac{tts} that use corpora of spoken language as training data. Spoken language in those cases is mostly represented as phonetic transcriptions. As phonetic transcriptions are strings of characters they are much easier to process for a machine than raw audio files. In order to represent spoken language as phonetic transcriptions, we need to know a lot about how spoken and written language relate. Research on \ac{asr} or \ac{tts} contributes to a better understanding of the relation between spoken and written language as both of the tasks involve mapping of spoken to written language. But still, this does not answer the question of how well written language represents language in general. The representative power of written text is much less studied. This is where this current thesis connects to cutting-edge research. I am going to present my attempt of studying a multilingual phonetic corpus and comparing it to its written-text version. 


\section{Research questions \& goals}
I will try to answer the following question as the result of my thesis: \textbf{Is it essential for the study of multilingual corpora to perform analyses on phonetic text (i.e.\ speech representations) rather than only written text?} This question addresses many different topics from the field of linguistics but also more technical aspects as I have mentioned in the paragraph above. None of these topics is trivial and the question cannot be answered easily. While in this thesis I cannot possibly discuss everything from the use of phonetic transcriptions up to the nature of human language use, the aim is to make a step into the direction of quantifying the representative power of written text. The goal is now to collect a corpus of phonetic transcriptions in various languages based on an already existing written corpus. Once I have the phonetic corpus, it is then possible to perform analyses on the phonetic corpus which can then be compared to the same analyses on the written corpus. In order to add a phonetic corpus to an already existing one, various steps need to be performed which are outlined below:
\begin{enumerate}
 \item \textbf{Data collection:} The given dataset contains no phonetic transcriptions of those 100 languages. The first step is to find already existing data. 
 \item \textbf{Computational model to create phonetic transcriptions:} As existing data will not be available in sufficient amounts to perform meaningful analysis, the next step is to actually create phonetic transcriptions of as many languages as possible of the corpus. This will require to create and train a computational model.
 \item \textbf{Test and analysis:} Once the transcriptions have been obtained, the newly created phonetic corpus can be analysed and tests can be performed on the phonetic data.
\end{enumerate}

The first two steps will already take up quite a lot of time which is hard to estimate in advance. It is not clear how much of the third step can be accomplished. I still think it is important to keep the bigger picture in mind and be aware of the overall importance of the topic. By performing the steps above I will have to tackle different challenges. Some of them are already known now, but there will be many more that will only become apparent while working on the tasks. 

\begin{enumerate}
\item What types of phonetic data is available and how can it be used?
\item Which computational models can be used to create phonetic transcriptions automatically?
\item How can we use phonetic features to create phonetic transcriptions?
\end{enumerate}

All of these challenges above are interesting and important, but the third challenge is of particular importance as it is a new approach to creating phonetic transcriptions. 

\section{Thesis structure}

The thesis is subdivided into six chapters of which the first chapter is this introduction. Chapter \ref{chap:ling-background} covers the linguistic basics that are necessary to understand the topic. It presents the linguistic foundation of phonetics and phonology and an introduction to corpus linguistics or rather corpus phonetics. Chapter \ref{chap:tech-background} sets the boundaries of the technical background. I will present an overview of the possibilities for automated creation of phonetic transcriptions. In chapter \ref{chap:data_collection}, I document my first practical experiments which are concerned with data collection. It contains a descriptions of the data I will later use to create models for phonetic transcriptions. In chapter \ref{chap:mwl} I will present one of my experiments where I compare the word length of phonetic and written text. Chapter \ref{chap:exp} presents my own experiments to create phonetic transcriptions of the corpus. I will present all models that I created and their results. Chapter \ref{chap:6_conclusion} summarizes my findings and presents ideas for future research.



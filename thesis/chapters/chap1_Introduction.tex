
\newchap{Introduction}
\label{chap:1_intro}

With the advent of technologies that can process huge amounts of data, many linguistic tasks that were originally very tiresome and expensive to do, can now be accomplished much faster. Well known examples for this branch called \ac{nlp} are machine translation or search engines. A lot of available tools and consequently research done in this area is concerned with written text. For many scenarios like machine translation large corpora of written text in many languages are collected that are used to train such models. Following, there is an ever-growing set of models that are trained on written text and are used to accomplish those text-based tasks. Often the goal is to reach or outperform human solutions to those various tasks. It is necessary that this training data represents language well enough for a machine to reach that performance. From a linguistic point of view, the questions comes up if focusing on written language only can ever represent human language adequately. Most of communication and daily language use happens through speaking. This is a first potential limitation to many (written-)language technologies. Another possible limitation is the concern if written text represents language in general well enough to draw significant conclusions. It is not easy to find out what characteristics of a language can be observed in written representations. There are technologies like \ac{asr} or \ac{tts} that require the mapping of written to spoken language. Spoken language in those cases is mostly represented as phonetic transcriptions as those are easier to process. Those research foci do contribute to the questions how spoken and written language relate. But this still does not answer the question of how well written language represents language in general. The representative power of written text is much less studied. This is where this current thesis connects to cutting-edge research. I am going present my attempt of studying a multilingual phonetic corpus and comparing it to its written-text version. I will try to answer the following question: \textbf{Is it essential for the study of multilingual corpora to perform analyses on phonetic text (i.e. speech representations) rather than only written text?} It becomes clear, when looking at these considerations, that there are a few huge topics addressed. None of these is trivial and can be answered easily. While this thesis cannot possibly discuss everything from the use of phonetic transcriptions up to the nature of human language use, the aim is to make a step into the direction of quantifying the representative power of written text. 


\section{Research questions \& goals}
The text group of the Language and Space lab at the University of Zurich maintains a project that provides a multilingual corpus consisting of 100 language text samples \citep{UniversityofZurich.19.07.2021}. Those 100 languages are meant to be representative for all the world's languages which is explained in more detail in section \ref{corpus}. It is therefore meant to give insight on relations, similarities, differences or properties of individual languages or language families. Specifically, their goal is to use quantitative methods like statistical modelling, machine learning and information theory to study language variation and compare languages. The goal is now to collect phonetic transcriptions of the corpus. The same analyses that are performed on the original written corpus can be performed on the phonetic texts and both can be compared. In order to add a phonetic corpus to the already existing one, various steps need to be performed which are outlined below:
\begin{enumerate}
 \item \textbf{Data collection:} The given dataset contains no phonetic transcriptions of those 100 languages. The first step is to find already existing data. 
 \item \textbf{Phonetic transcriptions:} As existing data will not be available in sufficient amounts to perform meaningful analysis, the next step is to actually create phonetic transcriptions of as many languages as possible of the corpus. 
 \item \textbf{Calculations and Analysis:} Once the transcriptions have been obtained, the newly created phonetic corpus can be analysed and calculations can be performed.
\end{enumerate}

By performing these steps I am aiming at answering the following two questions:

\begin{enumerate}
\item Is there any significant difference in comparing spoken or written languages?
\item Does written text represent language well enough to justify text-based research only?
\end{enumerate}

\section{Thesis structure}

The thesis is subdivided into \review{six} chapters including a final conclusion. Chapter \ref{chap:tech-background} sets the boundaries of the theoretical background. It presents the linguistic foundation of phonetics and phonology, an introduction to corpus linguistics or rather corpus phonetics and finally an overview of the possibilities for automated creation of phonetic transcriptions. Chapter \ref{chap:2_data} introduces to the struggle of data collection. It explains the various data types and how those can be used. Chapter \ref{chap:3_model} dives deeper into the possibilities for creating phonetic transcriptions and what models can be used to create those. Chapter \ref{chap:exp} presents my own experiments to create phonetic transcriptions of the corpus.



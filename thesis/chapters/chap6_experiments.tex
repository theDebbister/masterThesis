
\newchap{Experiments: Automatic Grapheme-to-Phoneme Conversion}
\label{chap:exp}
In this chapter, I present the experiments I conducted to obtain models for phonetic transcription. As a result of my first practical part in chapter \ref{chap:data_collection} I presented two datasets that I will now use to perform these experiments. The \ac{nws} corpus is much smaller which is why I will use it as a test dataset only. This means that I will train any model on the WikiPron dataset. As the \ac{nws} corpus is quite well known among linguists or at least among phoneticians, it will hopefully give interesting insights when testing the models on these short texts. Table \ref{tab:all_langs} shows all languages that I am using in my experiments. Note that WikiPron continuously adds data to their repository. This means that there might be new languages that are in the 100 language corpus and that I did not use. Additionally, there were languages which had a WikiPron dictionary that was smaller than 1,000 pairs. \citet{Ashby-Bartley.2021} that based their conclusion on similar WikiPron data have found that a dataset with 1,000 samples is not enough to train a decent model which is why I excluded these. If the transcription type of the \ac{nws} story is unknown, I only added it to the broad type of the WikiPron dictionary, such that there are no duplicates in the table. 

The model that I am using for my \ac{g2p} experiments is explained in section \ref{sec:cmu}. Setting it up was not very easy as there were issues with the tensorflow version and some other dependencies. Also, they do have a pre-trained model, but this uses a completely different transcription convention than IPA. So we cannot use this pre-trained model. But it is of course well possible to train  new models for the languages I am using. Generally, broad and narrow transcriptions are treated as separate languages and thus trained separately as well. The same is true for dialects if there is any information available. American and British English are trained in different models. In order to compare my results to already existing models, I will use the results of the \ac{sigm} tasks in 2020 and 2020. They present results for quite a few languages. Also, they use the same data type. They cleaned some of the data for some languages and made these datasets available. Whenever a filtered version is available I use that one as the basis for my experiments. 

\section{Reproducability}
All scripts that I used to preprocess, train and evaluate my models are found on my GitHub. I installed the model on my machine as specified on the model's GitHub\myfootnote{\url{https://github.com/cmusphinx/g2p-seq2seq}}. In order to reproduce my results I include a table below that lists all links to my relevant scripts and settings including the settings for the virtual environment. 

\section{CMUSphinx}
\label{sec:cmu}
For my personal experiments, I decided to use the CMUSphinx \ac{s2s} \ac{g2p} model. This model has been used in the \ac{sigm} 2021 task as part of the Dialpad ensemble \citep{gautam.2021}. It was not used on many languages but promised a good performance which is why I decided to use this model for this present thesis. The CMUSphinx model is a transformer-based \ac{s2s} model implemented with tensorflow. Unfortunatley, there is not a lot of information available online about the model. There exists a pre-trained version of the model for ac{g2p}, however, they use a transcription format other than \ac{ipalpha} which means it cannot be used for our dataset \citep{GitHub.03.02.2022}. 

\section{Training settings}
\label{sec:train-settings}
As I have shortly mentioned before, some WikiPron word lists have less than one thousand word pairs. I marked them as low resource and only used the high resource languages to train my models. There are different settings which I will use to train the models and analyse their performance. The settings are designed in a way that each setting introduces a new or more complex adaption of the previous setting. The first experiments are set up very simply and without much effort. Then I continuously add more complexity. Below I added a short description of each setting as an overview. I will add more details on the individual steps in separate sections. 

There are a few properties that are always the same for all models:
\begin{itemize}
\item The CMU model splits the data automatically if no specified otherwise. The automatic split is 85\% training data, 5\% development data and 10\% test data. I did not change this split.
\item The hyperparameters are left as default if not mentioned otherwise. This means that I did not adapt the model and used it as it can be found online. More on teh default settings can be found on the model's GitHub\myfootnote{\url{https://github.com/cmusphinx/g2p-seq2seq}}.
\item The training time for the short model for one language with 10,000 steps is about 20 minutes. The long models took another six hours to complete the training per language.  
\end{itemize}

\subparagraph{Setting 1: Baseline Short}
This is the most basic setting. I will train a model for each language to get a baseline result. The model is trained with the least amount of effort. The model is trained for the minimum number of steps which is 10,000 steps in this case. First, I trained it on those languages where I have results from the \ac{sigm} 2021 challenge. The results are compared in tables \ref{tab:baseline_short} and \ref{tab:baseline_long_sig}.

\subparagraph{Setting 2: Baseline Long}
This setting is similar to setting 1 except that the model is trained as long as possible for each language. All models have been trained for 200,000 steps. This setting is supposed to show if training for a considerably longer time changes the results a lot or not.

\subparagraph{Setting 3: Baseline clean short}
I will train another model that is the same like the Baseline Large, but I will use the cleaned WikiPron data. What I will clean is described below (section \ref{preprocess}).

\subparagraph{Setting 4: Baseline clean long}
The same as setting 3 except that the models were trained for as long as possible like in setting 2.

\subparagraph{Setting 5: Feature input version 1 short}
The final experiments will be with phonetic features as input. I use the cleaned WikiPron data from setting 3 and add the features to it. In section \ref{sec:feature_enc}, I explain how I encode the features. 

\subparagraph{Setting 6: Feature input version 1 long}
The same as setting 5, but again the models for trained for 200,000 steps like in setting 2.

\subparagraph{Setting 7: Feature input version 2 short}
This setting is the same as setting 5, but with a different version of the input feature data.

\subparagraph{Setting 8: Feature input version 2 long}
The same as setting 5, but again the models are trained for 200,000 steps like in setting 2 and the second feature version is used as input data as in setting 7.

\section{Preprocessing}
\label{preprocess}
In section \ref{sec:ipa} I talked about the incompleteness and difficulties of transcribing using the \ac{ipalpha}. How exactly a sound is mapped to an \ac{ipalpha} symbol also depends on whoever transcribes a particular text. The WikiPron data has been put together by many different people. There are conventions on how to add transcriptions by Wiktionary, but there still might be inconsistencies. Other than that, it is always possible that something is correct, but neural models just cannot handle it well. That said I will carefully examine and clean the datasets. There is preprocessing that is done for both datasets, further down I will describe some preprocessing that had to be done for each individual dataset.

The first analysis is a character based comparison. This means that I used the python segments library\myfootnote{\url{https://pypi.org/project/segments/}} to split up all the phonemes into Unicode characters. Then I compared those characters to the PHOIBLE database. As I have explained in section \ref{sec:phoible}, in can use the PHOIBLE databse to find phonemes that are uncommon for a specific language. While it is possible that a correctly used phoneme is not in PHOIBLE, it still gives a good overview of uncommon phonemes in the transcription or even points out mistakes in my data. This character based cleaning is done for both datasets. The more detailed list of what needs to be cleaned is found in table \ref{tab:preprocessing}.

There are two more general things that I removed from the datasets. The first is \textbf{tones}. The reason for that is that it is not possible to infer tones from graphemes. Although they are used to distinguish meaning, they are not written down, but usually just known. This means that there are grapheme sequences that look exactly the same and their transcription is the same as well \textit{except} for the tones. Example \ref{ex:chinese-tones} shows the same Chinese sign together with its transcription. The superscript number represent the tones. For none of the three examples are the tones the same. But if only shown the sign, there is absolutely no way to find out which tones are meant:

\begin{covsubexamples}
\label{ex:chinese-tones}
\item 
\begin{CJK*}{UTF8}{bsmi}
浸 \>\>\>\>\textipa{\t{tC}\super h i n} \textsuperscript{2}\textsuperscript{1}\textsuperscript{4}
\end{CJK*}
\item 
\begin{CJK*}{UTF8}{bsmi}
浸 \>\>\>\>\textipa{\t{tC}\super h i n} \textsuperscript{5}\textsuperscript{1}
\end{CJK*}
\item  
\begin{CJK*}{UTF8}{bsmi}
浸 \>\>\>\>\textipa{\t{tC}\super h i n} \textsuperscript{5}\textsuperscript{5}
\end{CJK*}
\end{covsubexamples}

For the reason of the ambiguity shown in example \ref{ex:chinese-tones}, I decided to exclude tones. There are different ways to represent tones and I excluded all of them. The second additional preprocessing step I took is to remove all \textbf{punctuation symbols}. This accounts mostly for grapheme version of the \ac{nws} short stories. For character-based modelling, punctuation does not add any valuable information as this only becomes relevant on a sentence basis. 

In example \ref{ex:cleaning} I list a few phonetic transcriptions before and after cleaning. 

\begin{covsubexamples}[preamble={This example shows stringe before and after cleaning them. The left-hand side shows the uncleaned version and the right-hand side the cleaned version. I added the language code and the transcription type and the original grapheme sequence in parentheses.}]
\label{ex:cleaning}
\item deu, narrow

\textipa{a: b @ n t m a: l \t{ts} a \textsubarch{I} t} $\rightarrow$ \textipa{a: b @ n t m a: l t s a \textsubarch{I} t} (Abendmahlzeit)
\item fin, broad

\textipa{A: l: o t A\super x}  $\rightarrow$ \textipa{A: l: o t A P} (aallota)
\item cmn, broad

\textipa{\t{tC}\super h i n} \textsuperscript{2}\textsuperscript{1}\textsuperscript{4} $\rightarrow$ \textipa{t C \super h i n}
\begin{CJK*}{UTF8}{bsmi}
(浸)
\end{CJK*}
\item eng uk, narrow

\textipa{@ l 3\textrhoticity  d Z I k} $\rightarrow$ \textipa{@ l @\textrhoticity  d Z I k} (allergic)
\end{covsubexamples}

\tab{tab:preprocessing}{The table shows what phonemes where changed or excluded and what the reason is for this preprocessing. All characters that were excluded are replaced by a NULL value. }{
\begin{tabularx}{\textwidth}{
| 	>{\raggedright\arraybackslash}l | 
	>{\raggedright\arraybackslash\hsize=.5\hsize\linewidth=\hsize}X | 
	>{\raggedright\arraybackslash}l  | 
	>{\raggedright\arraybackslash\hsize=1.5\hsize\linewidth=\hsize}X |}
\hline
\textbf{Phon.} & \textbf{Unicode name} & \textbf{Repl.} & \textbf{Explanation} \\
\hline
\hline
 \textipa{"} 					& \scriptsize{MODIFIER LETTER VERTICAL LINE} 				& NULL 						& \multirow[t]{6}{\hsize}{These are all \ac{ipalpha} suprasegmentals except the long and half long marker and the extra short (\textipa{: ; \u{}}). The reason why these were excluded is that they don't carry any meaning on the character level. The vertical lines, for example, mark intonation groups which only matter in a larger sentence or text context. There are a few rare occurrences of COMBINING VERTICAL LINE ABOVE which is probably meant to be MODIFIER LETTER VERTICAL LINE as they look similar. It is excluded as well.} \\
 
\textipa{""} 					& \scriptsize{MODIFIER LETTER LOW VERTICAL LINE}			& NULL						&  \\
\textipa{\textvertline} 		& \scriptsize{VERTICAL LINE} 								& NULL						&  \\
\textipa{\textdoublevertline} 	& \scriptsize{DOUBLE VERTICAL LINE} 						& NULL 						&  \\
\textipa{.} 					& \scriptsize{FULL STOP} 									& NULL 						&  \\
\textipa{\t*{}}					& \scriptsize{UNDERTIE} 									& NULL 						&  \\\hline
\textipa{\t{}}					& \scriptsize{COMBINING DOUBLE INVERTED BREVE} 				& NULL						&  \multirow[t]{2}{\hsize}{Both tie bars below and above are excluded in PHOIBLE\myfootnote{https://phoible.org/conventions} which is why I am excluding it as well. Put plainly, those do not add any additional information that cannot be derived otherwise.} \\
\textipa{\t*{}}					& \scriptsize{COMBINING DOUBLE BREVE BELOW} 				& NULL						&  \\\hline
\textipa{3\textrhoticity} 		& \scriptsize{LATIN SMALL LETTER REVERSED OPEN E WITH HOOK} & \textipa{@\textrhoticity} &  Both base letters are very similar vowels. It is just more common to use the latter than the former. \\\hline
g						 		& \scriptsize{LATIN SMALL LETTER G} 						& \textipa{g} 				&  The \ac{ipalpha} `\textipa{g}' has a different code point and is a different character than the typical keyboard small Latin `g'. This is just an \ac{ipalpha} decision. For some fonts the two characters do not look different, for some they do.  \\\hline
$\sim$							& \scriptsize{SWUNG DASH} 									& NULL 						& \multirow[t]{4}{\hsize}{All of characters make out less than 1\% of their respective dataset, most of the time it is less than 0.1\%. A close examination of the dataset and the Wiktionary transcription conventions for the respective language did not show any reason why to keep the phoneme. Note that the `v' for the tilde is only there for correct representation.}  \\
,							 	& \scriptsize{COMMA} 										& NULL 						&  \\
\textipa{\~v} 					& \scriptsize{TILDE} 										& NULL 						& \\
\textsuperscript{\textipa{@}}	& \scriptsize{MODIFIER LETTER SMALL SCHWA} 					& NULL &  \\\hline
\textsuperscript{x}				& \scriptsize{MODIFIER LETTER SMALL X} 						& \textipa{P}				&  The \textsuperscript{x} only occurred in the broad Finnish transcription and is used to denote possible gemination. In the narrow transcriptions there is a glottal stop instead. The occurrence of glottal stops and gemination follows the same rules. Therefore, for consistency, the gemination \textsuperscript{x} is mapped to a LATIN LETTER GLOTTAL STOP.   \\
\hline
( ) 						& \scriptsize{( SUPERSCRIPT ) [ LEFT | RIGHT ] PARENTHESIS}		& NULL							& Parentheses are used to denote optionality for phonemes or tones. WikiPron actually discards those but keeps the content\myfootnote{\url{https://github.com/CUNY-CL/wikipron}}. I will do the same for all parenthesis found.  \\\hline
\end{tabularx}}{Preprocessing}

\subsection*{NWS corpus}
Apart from cleaning the stories as described in table\ref{preprocess}, I had to transform them into dictionary format in order to be able to use them as testing data. In order to do that it is necessary to tokenize both orthographic and phonetic texts and then align them. As the number of tokens (when split naively at blanks) is not always the same for orthographic and phonetic text, it was necessary to align some of them manually. While this is not a problem when talking about languages that I know how to pronounce, it is a bit tricky for languages completely unknown to me. Luckily there are tools online that provide a rough pronunciation of a word in a given language or even a phonetic transcription (although rarely in \ac{ipalpha})\myfootnote{For most languages I could use Google translator (\url{https://translate.google.com/?hl=de&sl=ko&tl=de&op=translate}), for languages like Hebrew, I could ask someone who knows the language to help me out.}. 

Chinese and Thai required special tokenization. To tokenize Chinese and Thai I used \textit{polyglot}\myfootnote{\url{https://polyglot.readthedocs.io/en/latest/}}, a Python library.

Apart form the characters that I cleaned out in table \ref{tab:preprocessing}, there were a few more characters that needed to be cleaned. 
As the models cannot handle characters that are not in their vocabulary (in the vocabulary are all characters that were in the training data), I needed to remove the unknown characters to be able to evaluate the models on those stories. However, for some characters, they were in the vocabulary, but the input data was in the wrong Unicode normalization form as described in section \ref{sec:unicode_ipa}. For example, some characters were precomposed characters while they were not precomposed in the \ac{nws} stories I used to test the model. Both characters in the model's vocabulary and the \ac{nws} stories look exactly the same but the model (or any other text processing application) treats them as two completely different characters because they have a different code point. 


\subsection*{WikiPron} 
As the CMU model does not expect the graphemes to contain any whitespace, I replaced any whitespace in the input with underscores. This was necessary only for Vietnamese. In a first run, I did not do this. When running the evaluation on the Vietnamese model, it did not work but showed an error message. As the model creates a vocabulary file, I had a look at it which revealed the following: 
\begin{itemize}
\item The model does not actually split the input file at tab characters, but splits it at spaces and then uses the first item of the resulting list as input grapheme and the rest as a list containing the output phonemes. Consequently, if the input grapheme contains a white space, everything following it will be in the phoneme part. Part of the input will be treated as a phoneme character in the training which results in a huge vocabulary and wrong predictions.
\end{itemize}

The alternative to removing the white space would have been to split the grapheme at white space and align the phonemes. However, this is firstly much more work as it is not clear where to split the phonemes and it might be that the word on its own is pronounced differently. 

In Japanese there is the superscript letter $^\beta$. This actually means compression, which is a special type of rounding\myfootnote{\url{https://en.wikipedia.org/wiki/Roundedness}}. This is one of these things where the difficulties of transcribing texts becomes clear. This special case is not reflected in PHOIBLE nor in the official \ac{ipalpha} but this does not mean it is wrong. As it is quite common in the Japanese data, I decided to keep it.

A last thing I did to clean the datasets was to exclude duplicate graphemes with different pronunciations. Although ambiguities and multiple possible pronunciations for one word are linguistically speaking very common, it is not possible for a neural model to distinguish such cases without any context. As \ac{g2p} modelling happens on character basis and not on word basis there is no context available in our case that could account for at least some ambiguities. Also, in the WikiPron data that was preprocessed for the \ac{sigm} task, duplicates were excluded as well. It makes sense to clean all datasets in the same way.


\section{Feature encoding}
\label{sec:feature_enc}
In order to incorporate the phonetic features into the dataset, we decided to add what we refer to as `flags' to the phonemes. Generally, the idea is to encode the phonetic features that PHOIBLE provides and add them to the WikiPron dataset.  There are still a few phonemes that are not in the PHOIBLE set that are in my data. These will jsut not b encoded in the following experiments. An exception is the length marker. There are a few long vowels that are not like that in the PHOIBLE data. As there is the `long' feature, it is easily possible to use use the base character feature vector and mark the `long' as `+'. This is the only thing I changed for the following experiments. All other features vectors are just used as is. I encoded the features in a jupyter notebook which is in my GitHub repository\myfootnote{\url{https://github.com/theDebbister/masterThesis/blob/main/data/MA_Encode_features.ipynb}}.

I experimented with several things before I trained a first set of models. I will list the steps of my experiments in the following. For all feature experiments I used the cleaned WikiPron data.

\subsection*{Version 1}

The first thing I tried was to add two flags to every phoneme. The idea is to use the PHOIBLE feature set for each of the phonemes and encode the information that is found in those features. As there are 37 features for each phoneme they allow to encode much more information than one single character can. By encoding the phonetic features we can add more information that the model can use to create the grapheme sequences. For example: in some languages, there are vowel-consonant patterns. In German or also English a word typically needs at least one vowel and it is uncommon to have consonant sequences longer than three letters. If it was possible to encode for each phoneme if it was a vowel or a consonant, the model could abstract that information more easily. 

In order to add this phonetic information to the datasets, I performed the following steps:
\begin{description}
    \item[\textsc{1. step}] I split the list of all 37 PHOIBLE features (see section \ref{sec:phoible}) into two sets of PHOIBLE features. After this step, I had two sets. One containing the first half of all features, the other one containing the second half of all features.
    \item[\textsc{2. step}] For each set, I used two different letters to encode all possible feature combinations that can be obtained by combining all features in the set with all other features in the same set. If there had been two features, `syllabic' and `consonantal', in the set, there are nine possible combinations (refer to section \ref{sec:phoible} to understand the meaning of the values for each features). Table \ref{tab:feature-encoding} shows how this step looks like for a small example.
    To each of these combinations I assigned two different capital letters.
    \item[\textsc{3. step}] Once I completed the above step, I had to sets of strings, that I could combine to encode all possible combinations of features. Each string represents a different combination for one subset of the features.
    \item[\textsc{4. step}] Then, for each phoneme, I could assign it to two strings. The first string represents the first half of its features, the second string represents the second half of its features.
\end{description}

    \tab{tab:feature-encoding}{This table shows an example of how the feature encoding works. In this case there are only two features involved. Nonetheless, the procedure works the same for more features.}{
    \begin{tabular}{|ll|l|}
    \hline
        \textbf{syllabic} & \textbf{consonantal} & \textbf{feature encoding} \\\hline\hline
        +        & -           & AA       \\
        +        & 0           & AB       \\
        +        & +           & AC       \\
        -        & -           & AD       \\
        -        & 0           & BA       \\
        -        & +           & BB       \\
        0        & -           & BC       \\
        0        & 0           & BD       \\
        0        & +           & CA       \\\hline     
        \end{tabular}
        }{Feature encoding}
After completing the above steps, I get an output that looks similar to the example below. The combination of the two flags is be unique for each phoneme.

\begin{covexamples}
\item \label{ex1} \textipa{p} AB CD \textipa{k\super h} AA BC
\item \label{ex2} \textipa{k\super h} AA BC  \ipa{U} BF CC
\end{covexamples}

A problem with this approach is that the sequences get really long if all phonemes are represented by a string of length three.  Examples \ref{ex1} and \ref{ex2} show that the phoneme sequence gets three times longer. Originally there were only two segments separated by a white space while after the feature flags are added there are six segments. However, the CMU model separates the input phoneme sequences at white spaces and cuts off every segment after 30 segments. Each of the segments gets added to the vocabulary. This means that only phoneme sequences that are no longer than 10 segments will be represented in full when for each phoneme two additional feature flags are added as in the example above. Many original phoneme sequences are longer than 10 segments. It is possible to increase the limit of the model when to cut off segments. However, 30 segments is already quite long. There are not many words that are longer than 30 segments. There is no point in setting this limit too high as neural models like the CMU model still experience difficulties in processing sequences that are too long. Not only the sequence length is influenced but also the vocabulary size. Each flag will be added to the vocabulary which increases the vocabulary by a lot.  

It is therefore not surprising that the model did not perform well. In fact, the \ac{wer} was about double the \ac{wer} of the long model trained on the uncleaned data. Given this results, I decided not to train any more models and look for another way of how to encode the features.

\subsection*{Version 2}
For the second attempt at encoding phonetic features I tried a different approach. Instead of adding two flags, I added only one flag after each phoneme. Also, this time the idea was not to add a unique flag to each phoneme but to encode only some features for all phonemes. This means that for some phonemes the same flag was added. As the phonemes were not replaced by the flags, but the phonemes were kept in the text, there is no information lost. 

The intuition behind this second approach is to encode high-level patterns in phoneme sequences. For example, as vowels typically have some overlapping features, very similar vowels will be encoded using the same flag. Again, in order to enrich my data with the features, I followed specific steps which I will describe in more detail below: 

\begin{description}
    \item[\textsc{1. step}] As a very first step, I collected a list of all unique phonemes in PHOIBLE together with their features.
    \item[\textsc{2. step}] In contrast to the first feature version, for the second feature version I computed the features based on the phoneme inventory of each language. Consequently, I collected a set all PHOIBLE phonemes for \textit{each language separately}. As I am going to train a model for each language separately, I can encode the features language specific as long as the logic behind it is the same to be able to compare them. All of the following steps are conducted for each language separately.
    \item[\textsc{3. step}] As a next step, I collected a set of all phonemes for each language that are found in the training data for the models which is the WikiPron data in my case. 
    After this step, for each language I have two sets of phonemes:
    \begin{enumerate}
        \item one set for all phonemes in the WikiPron training data for the respective language
        \item one set for all phonemes in PHOIBLE for the respective language
    \end{enumerate}
    Taking the intersection of these two sets gives me the set of phonemes which are found in PHOIBLE for that language.
    \item[\textsc{4. step}] For each phoneme in the training set, I can get the PHOIBLE feature vector for that phoneme. If a phoneme is not found in PHOIBLE but in the training data, I just ignore that phoneme for the next steps.
    \item[\textsc{5. step}] This step is the key to the current feature version. I decide what features to encode for each language performing this step. In order to choose appropriate features for each language, I calculated the Pearson correlation between the features for each language. I only used those features whose correlation with all other features is not above a certain threshold. For this experiment I set the threshold to be 0.5. If the threshold was too high, for many languages, no correlating features were found. This is not surprising as features in general do not make a lot of sense if they are all telling the same thing. This is why I set the threshold relatively low which means I kicked out a lot of features. 
    
    The reason for this is that all languages only use a relatively small subset of phonemes. This means that many features are not important for a language as those features are not used to distinguish meaning. In order to recognize high-level patterns, only the most distinctive features are important.
    \item[\textsc{6. step}] Once that I have all the features for one language that I want to encode, I can actually encode them. This step is the same as for the first feature version. I get all combinations of the features to encode for each language (similar as in table \ref{tab:feature-encoding}) and assign it a different capitalized two-letter string. Table \ref{tab:f2-strings} lists a few example strings for how the input data changed after enriching it with features. In table \ref{tab:enc_features}, I list the features that were encoded for each language. 
\end{description}

\tab{tab:f2-strings}{This table shows example for the German broad WikiPron data for the feature encoding version 2. It shows the graphems, the unchanged phoneme sequence and the phoneme sequence with features.}{
\begin{tabular}{|lll|}
\hline
\textbf{Grapheme}   & \textbf{Phoneme}           & \textbf{Phonemes with features}                    \\\hline\hline
a          & \textipa{P a:}                 & \textipa{P} AK \textipa{a:} AM                                \\
aa         & \textipa{a:}                   & \textipa{a:} AM                                     \\
aachen     & \textipa{a: x @ n}             & \textipa{a:} AM \textipa{x} AJ \textipa{@} AL \textipa{n} AG                      \\
aachener   & \textipa{a: x @ n 5}           & \textipa{a:} AM \textipa{x} AJ \textipa{@} AL \textipa{n} AG \textipa{5} AG          \\
aachenerin & \textipa{a: x @ n @ K I n}     & \textipa{a: x @ n @ K I n} aː AM x AJ ə AL n AG ə AL ʁ AJ ɪ AL n AG  \\
...        & ...                            & ...                                       \\
übungen    & \textipa{P y: b U N @ n}       & \textipa{P y: b U N @ n} ʔ AK yː BA b AH ʊ AN ŋ AG ə AL n AG       \\
übungsbuch & \textipa{y: b U N s b u: x}    & \textipa{y: b U N s b u: x} yː BA b AH ʊ AN ŋ AG s AJ b AH uː BA x AJ \\
üppig      & \textipa{Y p I \|)c} ̧          &\textipa{Y p I \|)c}  ʏ AN p AH ɪ AL ç                         \\
üsselig    & \textipa{Y z @ l I \|)c}̧       &\textipa{Y z @ l I \|)c} ʏ AN z AJ ə AL l AE ɪ AL ç               \\
œuvre      & \textipa{œ: v K @} œː v ʁ ə    & \textipa{œ: v K @} œː BA v BC ʁ AJ ə AL       \\\hline              
\end{tabular}
}{Example data feature version 2}


While encoding these features, I noticed that some phonemes are not listed as individual phonemes in PHOIBLE but are listed as allophones of one phoneme that is in there. This means that some broad transcriptions contain allophones which, ideally, should not be the case. Also, it means that I did not have features for these phonemes which means that those were not encoded. It would be possible to replace all allophones for each language with the respective phoneme. However, this would have gone beyond the scope of this present thesis.

\tab{tab:enc_features}{The table shows all features that were encoded for each language. Some of them are encoded for all languages, those are at the top of the table.}{
\vspace{-0.5cm}
\begin{tabularx}{1.1\textwidth}{|llX|}
\hline
\textbf{ISO 396-3} & \textbf{Type} & \textbf{Features}                                                                                                                  \\
\hline
\hline
all langs     & -             & tone, stress, syllabic, short, consonantal, tap, trill, lateral, labial, epilaryngealSource, spreadGlottis, loweredLarynxImplosive \\
\hline
\hline
cmn                & broad         & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
deu                & narrow        & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
ell                & broad         & long, sonorant, continuant, delayedRelease, nasal, coronal, dorsal, constrictedGlottis, raisedLarynxEjective                       \\
eng uk             & broad         & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
eng uk             & narrow        & long, continuant, nasal, constrictedGlottis                                                                                        \\
eng us             & broad         & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
eng us             & narrow        & long, continuant, nasal, constrictedGlottis                                                                                        \\
eus                & broad         & long, continuant, delayedRelease, nasal, constrictedGlottis, raisedLarynxEjective                                                  \\
fin                & broad         & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
fin                & narrow        & long, continuant, delayedRelease, nasal, constrictedGlottis, raisedLarynxEjective                                                  \\
fra                & broad         & long, nasal, constrictedGlottis, raisedLarynxEjective                                                                              \\
hin                & broad         & constrictedGlottis, raisedLarynxEjective                                                                                           \\
hin                & narrow        & nasal, constrictedGlottis, raisedLarynxEjective                                                                                    \\
ind                & broad         & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
ind                & narrow        & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
jpn                & narrow        & long, delayedRelease, coronal, dorsal, constrictedGlottis, raisedLarynxEjective                                                    \\
kat                & broad    & long, continuant, delayedRelease, nasal, coronal, constrictedGlottis                                                               \\
kor                & narrow        & continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                        \\
mya                & broad    & long, delayedRelease, nasal, raisedLarynxEjective                                                                                  \\
rus                & narrow        & long, continuant, nasal, coronal, dorsal, constrictedGlottis, raisedLarynxEjective                                                 \\
spa ca             & broad         & long, sonorant, continuant, delayedRelease, nasal, dorsal, constrictedGlottis, raisedLarynxEjective                                \\
spa ca             & narrow        & long, sonorant, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                        \\
spa la             & broad         & long, sonorant, continuant, nasal, coronal, dorsal, constrictedGlottis, raisedLarynxEjective                                       \\
spa la             & narrow        & long, sonorant, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                        \\
tgl                & broad         & long, continuant, delayedRelease, nasal, constrictedGlottis, raisedLarynxEjective                                                  \\
tgl                & narrow        & long, sonorant, continuant, nasal, dorsal, constrictedGlottis, raisedLarynxEjective                                                \\
tha                & broad         & nasal, constrictedGlottis, raisedLarynxEjective                                                                                    \\
tur                & broad         & long, continuant, nasal, constrictedGlottis, raisedLarynxEjective                                                                  \\
tur                & narrow        & long, nasal, constrictedGlottis, raisedLarynxEjective                                                                              \\
vie                & narrow        & long, nasal, coronal, constrictedGlottis, raisedLarynxEjective                                                                     \\
zul                & broad         & continuant, delayedRelease, nasal, coronal, constrictedGlottis, raisedLarynxEjective  \\
\hline                                            
\end{tabularx}
\vspace{-0.2cm}
}{Encoded features per language}


\section{Results}

For the evaluation of my models, I first used the test sets. The test set is a subset of the entire dataset as explained in section \ref{sec:train-eval}. In addition, I tested each model on the respective \ac{nws} story if available in that language. Below, I first discuss the results separately for the short (setting 1, setting 3 and setting 5) and the long (setting 2, setting 4 and setting 6) models. I will mostly compare the \ac{wer} scores of the model, as this is common in research. Also, the \ac{per} scores are a bit harder to interpret, as the edit distance it involves is not very intuitively understandable as there are many different ways of how to get to the same result. 

To evaluate the models that were trained on data containing feature flags, I cleaned out the features to calculate the scores on the predicted phonemes only. Leaving in the feature flags would make it difficult to compare it to other models' results.

\subsection{Results short models}
Table \ref{tab:res_short_full} shows the results for all models that I trained for 10,000 steps. It is interesting that the results for all three models are actually very similar. I was surprised by the fact that for a vast majority of the languages the version trained on the cleaned data did not improve the performance although the cleaning was supposed to make the data more consistent. For example, excluding duplicate words with different phoneme sequences should remove some ambiguities. However, it is also possible that it actually introduced more ambiguities as I randomly took one version of the duplicates which might be an uncommon one. An interesting result is the one for Zulu (zul). The cleaned model performed a lot better than the other two. As this result seems rather strange, I will have a close look at the data again. Manually comparing the test prediction for Zulu to the reference phonemes (there are only 167 words in the test set), reveals that the model almost exclusively got the tones wrong. Those were excluded in the cleaned version which might explain the wide gap. Finnish (fin) had a very similar result but the gap was not as large.

\tab{tab:res_short_full}{This table shows the results for setting 1 (BS), setting 3 (BS-clean) and setting 7 (F2).These are the short models.}{
\begin{tabularx}{\textwidth}{|l|X||r|r||X|X||r|r|}
\hline
\textbf{Iso 639-3} & \textbf{Type WikiPron} & \textbf{WER BS}       & \textbf{PER BS} & \textbf{WER BS-clean} & \textbf{PER BS-clean} & \textbf{WER F2} & \textbf{PER F2} \\ \hline \hline
cmn    & broad  & 19.6 & 4.5  & \textbf{18.1} & 5.0  & 20.3 & 6.0  \\
deu    & broad  & 40.3 & 5.4  & \textbf{38.1} & 5.3  & 40.2 & 6.6  \\
deu    & narrow & \textbf{52.1} & 7.5  & 59.9 & 9.4  & 59.9 & 10.4 \\
ell    & broad  & \textbf{9.8}  & 0.9  & 10.4 & 1.0  & 11.5 & 1.1  \\
eng us & broad  & 54.4 & 10.6 & \textbf{53.0} & 10.8 & 57.7 & 12.1 \\
eng us & narrow & 84.6 & 32.0 & \textbf{84.2} & 31.8 & 86.6 & 32.2 \\
eng uk & broad  & \textbf{48.6} & 9.5  & 50.0 & 10.1 & 51.8 & 11.1 \\
eng uk & narrow & \textbf{88.8} & 32.2 & 93.8 & 34.5 & 90.5 & 31.2 \\
eus    & broad  & \textbf{19.4} & 2.8  & 22.8 & 2.8  & 23.6 & 4.0  \\
fin    & broad  & \textbf{5.8}  & 0.4  & 11.3 & 1.3  & 11.5 & 3.1  \\
fin    & narrow & 14.3 & 1.0  & \textbf{7.1}  & 0.7  & 17.1 & 4.3  \\
fra    & broad  & \textbf{7.2}  & 1.0  & 8.8  & 1.3  & 9.1  & 1.6  \\
hin    & narrow & 8.4  & 1.4  & 10.1 & 1.6  & \textbf{8.3}  & 1.3  \\
hin    & broad  & \textbf{5.6} & 1.2  & 7.3  & 1.2  & 6.9  & 1.2  \\
ind    & broad  & \textbf{35.3} & 5.3  & 36.3 & 4.8  & 38.6 & 5.7  \\
ind    & narrow & \textbf{43.5} & 5.5  & 44.1 & 6.0  & 44.3 & 6.1  \\
jpn    & narrow & \textbf{6.6}  & 0.9  & 6.7  & 0.9  & 6.8  & 1.1  \\
kat    & broad  & \textbf{0.3}  & 0.2  & 1.1  & 0.2  & 1.7  & 0.9  \\
kor    & narrow & 28.7 & 4.6  & 27.0 & 4.4  & \textbf{26.7} & 4.9  \\
mya    & broad  & 34.2 & 7.4  & \textbf{34.1} & 7.1  & 87.5 & 18.8 \\
rus    & narrow & \textbf{14.8} & 1.7  & 16.4 & 1.9  & 22.0 & 3.6  \\
spa ca & broad  & \textbf{2.3}  & 0.3  & 2.7  & 0.4  & 5.3  & 1.5  \\
spa ca & narrow & \textbf{3.6}  & 0.5  & 9.1  & 1.5  & 8.4  & 1.6  \\
spa la & broad  & \textbf{2.6}  & 0.3  & 2.8  & 0.5  & 4.4  & 1.0  \\
spa la & narrow & 3.3  & 0.4  & \textbf{3.2}  & 0.3  & 5.4  & 0.8  \\
tgl    & broad  & \textbf{33.3} & 5.9  & 33.4 & 5.1  & 34.8 & 5.6  \\
tgl    & narrow & \textbf{42.9} & 6.6  & 51.1 & 7.6  & 50.3 & 7.2  \\
tha    & broad  & 14.1 & 2.9  & \textbf{13.0} & 2.8  & 15.3 & 3.7  \\
tur    & broad  & \textbf{52.8} & 8.0  & 53.4 & 7.9  & 54.3 & 8.6  \\
tur    & narrow & 57.9 & 8.4  & 53.9 & 7.9  & \textbf{52.0} & 8.5  \\
vie    & narrow & \textbf{3.0}  & 1.3  & \textbf{3.0}  & 1.5  & 4.3  & 2.0  \\
zul    & broad  & 61.7 & 11.5 & \textbf{10.4} & 1.1  & 95.1 & 13.7 \\ \hline
\end{tabularx}
}{Results: short models}

Table \ref{tab:baseline_short} shows a comparison between my short models and the \ac{sigm} models. The results for the shared task are the same no matter to which of my models I compare them. As we will see in the next section, my long models performed better than the short ones o presented in the current section. Therefore, I will not discuss the results at this point for the short models compared with the shared task. I will only discuss my results of the long models compared with the \ac{sigm} models.

\tab{tab:baseline_short}{The table shows the \ac{wer} results for the best of my short models compared with the \ac{sigm} 2020 and 2021 results. For each language the best score is reported no matter what year or what model from the \ac{sigm} task. My models and the \ac{sigm} models were trained on WikiPron data. However, the \ac{sigm} data was preprocessed in a slightly different way which means that the results are not directly comparable. All references for the \ac{sigm} models can be found in tables \ref{tab:sota} and \ref{tab:sota_table_long}.}{
\begin{tabular}{|rrrr|}
\hline
\textbf{ISO396-3} & \textbf{BS WER} &  \textbf{SIG WER} & \textbf{Transcription type} \\
\hline
\hline
eng (us) & 53.00 & \textbf{37.43} & broad \\
fra & 7.20 &  \textbf{5.11} & broad \\
ell & \textbf{9.80} &  18.67 & broad \\
kat & 0.30 &  \textbf{0.00} & broad \\
hin & 5.60 &  \textbf{5.11} & broad \\
jpn & 6.60 &  \textbf{4.89} & narrow \\
kor & 26.70 &  \textbf{16.20} & narrow \\
vie & 3.00 &  \textbf{0.89} & narrow \\
\hline
\end{tabular}
}{Short models comparison}

\subsection{Results long models}

Table \ref{tab:res_long_full} shows the results for all models that I trained for 200,000 steps. The results and the differences between the models are very similar to the short models. The observation for Zulu (zul) is still the same. In fact, the model on the uncleaned data performed even worse than its short equivalent.

The largest improvement from the short to the long models was for Russian (rus). Russian had by far the largest dataset with more than 400,000 samples. It makes sense that for this large dataset, the model takes more time to extract all the relevant information. 

\tab{tab:res_long_full}{This table shows the results for setting 2 (BS), setting 4 (BS-clean) and setting 8 (F2). These are the long models}{
\begin{tabularx}{\textwidth}{|l|X||r|r||X|X||r|r|}
\hline
\textbf{Iso 639-3} & \textbf{Type WikiPron} & \textbf{WER BS}       & \textbf{PER BS} & \textbf{WER BS-clean} & \textbf{PER BS-clean} & \textbf{WER F2} & \textbf{PER F2} \\ \hline \hline
cmn       & broad         & 17.6         & \textit{3.9}    & \textbf{17.0}         & \textit{4.0}          & 18.1   & \textit{4.4}    \\
deu       & broad         & \textbf{37.1}& \textit{4.8}    & 38.1         & \textit{5.0}          & 38.6   & \textit{5.9}    \\
deu       & narrow        & \textbf{52.2}& \textit{7.1}    & 56.9         & \textit{8.4}          & 55.9   & \textit{9.4}    \\
ell       & broad         & \textbf{7.1} & \textit{0.6}    & 9.0          & \textit{0.7}          & 9.1    & \textit{0.8}    \\
eng us    & broad         & \textbf{50.7}& \textit{9.5}    & 51.2         & \textit{10.2}         & 51.3   & \textit{10.4}   \\
eng us    & narrow        & 84.6         & \textit{31.4}   & \textbf{84.2}& \textit{33.1}         & 84.9   & \textit{32.3}   \\
eng uk    & broad         & \textbf{45.5}& \textit{8.6}    & 47.5         & \textit{9.2}          & 47.6   & \textit{9.7}    \\
eng uk    & narrow        & \textbf{90.3}& \textit{30.2}   & 94.8         & \textit{35.3}         & 93.7   & \textit{34.2}   \\
eus       & broad         & 21.2         & \textit{2.7}    & \textbf{19.6}& \textit{2.3}          & 21.7   & \textit{3.2}    \\
fin       & broad         & \textbf{2.8} & \textit{0.2}    & 3.3          & \textit{0.3}          & 8.9    & \textit{3.1}    \\
fin       & narrow        & \textbf{3.2} & \textit{0.3}    & 3.9          & \textit{0.4}          & 9.0    & \textit{3.5}    \\
fra       & broad         & \textbf{5.3} & \textit{0.7}    & 5.8          & \textit{0.8}          & 5.9    & \textit{0.8}    \\
hin       & narrow        & \textbf{7.7} & \textit{1.2}    & 8.4          & \textit{1.4}          & 8.4    & \textit{1.3}    \\
hin       & broad         & \textbf{4.4} & \textit{0.7}    & 6.4          & \textit{1.0}          & 6.3    & \textit{1.0}    \\
ind       & broad         & 37.9         & \textit{5.3}    & \textbf{34.9}& \textit{5.3}          & 39.3   & \textit{6.1}    \\
ind       & narrow        & 43.1         & \textit{5.4}    & \textbf{43.0}& \textit{5.6}          & 43.1   & \textit{5.6}    \\
jpn       & narrow        & \textbf{6.5} & \textit{0.6}    & 6.8          & \textit{0.6}          & 6.6    & \textit{0.8}    \\
kat       & broad         & \textbf{0.0} & \textit{0.0}    & \textbf{0.0} & \textit{0.0}          & 1.0    & \textit{0.8}    \\
kor       & narrow        & \textbf{23.4}& \textit{4.1}    & 25.3         & \textit{4.4}          & 25.8   & \textit{4.6}    \\
mya       & broad         & \textbf{35.1}& \textit{6.5}    & 36.0         & \textit{6.9}          & 88.0   & \textit{17.4}   \\
rus       & narrow        & \textbf{1.9} & \textit{0.2}    & 2.4          & \textit{0.3}          & 5.0    & \textit{1.5}    \\
spa ca    & broad         & \textbf{1.1} & \textit{0.1}    & 1.3          & \textit{0.1}          & 2.2    & \textit{0.7}    \\
spa ca    & narrow        & 2.3          & \textit{0.3}    & \textbf{2.2}          & \textit{0.3}          & 2.8    & \textit{0.6}    \\
spa la    & broad         & \textbf{1.4} & \textit{0.1}    & 1.5          & \textit{0.2}          & 1.9    & \textit{0.6}    \\
spa la    & narrow        & \textbf{2.6} & \textit{0.3}    & 2.7          & \textit{0.3}          & 2.7    & \textit{0.4}    \\
tgl       & broad         & \textbf{28.4}& \textit{4.6}    & 31.2         & \textit{5.2}          & 33.9   & \textit{5.0}    \\
tgl       & narrow        & \textbf{45.5}& \textit{6.4}    & 47.3         & \textit{7.0}          & 48.6   & \textit{6.9}    \\
tha       & broad         & 12.5         & \textit{2.6}    & \textbf{11.1}         & \textit{2.5}          & 12.1   & \textit{3.1}    \\
tur       & broad         & 50.6         & \textit{7.8}    & 52.3         & \textit{7.5}          & \textbf{49.1}   & \textit{7.2}    \\
tur       & narrow        & 55.1         & \textit{7.6}    & 55.6         & \textit{8.3}          & \textbf{54.8}   & \textit{8.0}    \\
vie       & narrow        & \textbf{1.5} & \textit{0.8}    & 1.6          & \textit{0.8}          & 2.6    & \textit{1.5}    \\
zul       & broad         & 65.9         & \textit{10.7}   & \textbf{9.8}          & \textit{0.9}          & 91.4   & \textit{12.0}   \\ \hline
\end{tabularx}
}{Results: long models}

Table \ref{tab:baseline_long_sig} shows a comparison of the best long model compared with the best \ac{sigm} shared task result. For three of those eight languages, my model reached a better or the same score. The \ac{sigm} Korean model performed quite a lot better than mine which can be explained by the fact, that the \ac{sigm} model was not trained on the Korean logograms but those were converted to hangul characters which is a Latin alphabet representation of Korean. As explained in section \ref{section:sig}, this can lead to better results as it reduces the vocabulary size. My result for English is a lot worse as well. Generally, English (as well as German) seems to be a language that is relatively difficult to pronounce as the results are often a lot worse than those of other languages. In the present case, the difference might result from the fact that the \ac{sigm} model was an ensemble and it was only trained for English and therefore optimized to work for the English language. My model performs better on Modern Greek. This is an excepted result, as the model from the \ac{sigm} task was trained on only 800 samples. I had more than 10,000 samples. This aligns with cutting edge research, as \citet{Ashby-Bartley.2021} have found that 800 samples are not enough to train a decent model. My results confirm this finding as my model trained on 10,000 samples of the same data was \textit{considerably} better than the model trained on only 800 samples. The only difference between the training sets of the models was that I applied a few different preprocessing steps (see section \ref{preprocess}). 

\tab{tab:baseline_long_sig}{The table shows the \ac{wer} results for the best of my long models compared with the \ac{sigm} 2020 and 2021 results. For each language the best score is reported no matter what year or what model from the \ac{sigm} task. My models and the \ac{sigm} models were trained on WikiPron data. However, the \ac{sigm} data was preprocessed in a slightly different way which means that the results are not directly comparable. All references for the \ac{sigm} models can be found in tables \ref{tab:sota} and \ref{tab:sota_table_long}.}{
\begin{tabular}{|rrrr|}
\hline
\textbf{ISO396-3} & \textbf{BS WER} & \textbf{SIG WER} & \textbf{Transcription type} \\
\hline
\hline
eng (us) & 50.70 &  \textbf{37.43} & broad \\
fra & 5.30 &  \textbf{5.11} & broad \\
ell & \textbf{7.10} &  18.67 & broad \\
kat & \textbf{0.00} &  \textbf{0.00} & broad \\
hin & \textbf{4.40} &  5.11 & broad \\
jpn & 6.50 &  \textbf{4.89} & narrow \\
kor & 23.40 & \textbf{16.20} & narrow \\
vie & 1.50 &  \textbf{0.89} & narrow \\
\hline
\end{tabular}
}{Long models comparison}



\subsection{Results NWS tests}
Table \ref{tab:nws_test} presents the results for the tests on the \ac{nws} stories. The models do not perform very well on these stories in general. Concering the model types, there is no real pattern in how they perform. The clean models seems to perform a bit better than the other two types. Most of them reach the best performance at least once. Also, as the texts are really short, the performance is often very similar.

Still, when compared to the coverage experiment in section \ref{sec:coverage}, many \ac{g2p} models perform quite a lot better. Although this is not surprising, it it good that we can confirm that those models are able to extract a lot of information. What is surprsing is that there are a few languages link broad Hindi (hin) or Vietnamese (vie) where the \ac{wer} of the \ac{g2p} models is higher or very similar to the coverage experiment. Neither of these models performed particularly bad when tested on the WikiPron test set (\ac{wer} hin: 4.4, \ac{wer} vie: 1.5), in fact the results were very good. In addition, both training sets contain more than 10,000 samples. The most plausible explanation is probably that the transcription conventions are just very different. 

\begin{center}
\tab{tab:nws_test}{In this table I present the results for all models when they predicted the NWS stories. The models are explained in section \ref{sec:train-settings} where I explain the different training settings. `Long' corresponds to setting 2, `short' to setting 1, `Long clean' to setting 4, `Short clean' to setting 3, `F2 long' to setting 8 and `F2 short' to setting 7.}{
\hspace*{-1.5cm}
\begin{tabularx}{1.2\textwidth}{|Xl||ll||ll||ll||ll||ll||ll|}
\hline
\textbf{ISO 396-3} & \textbf{Type} & \multicolumn{2}{c||}{\textbf{Long}} & \multicolumn{2}{c||}{\textbf{Short}} & \multicolumn{2}{c||}{\textbf{Long clean}} & \multicolumn{2}{c||}{\textbf{Short clean}} & \multicolumn{2}{c||}{\textbf{F2 long}} & \multicolumn{2}{c|}{\textbf{F2 short}} \\ \hline
                   &               & \textbf{WER}       & \textbf{PER}     & \textbf{WER}       & \textbf{PER}      & \textbf{WER}        & \textbf{PER}      & \textbf{WER}        & \textbf{PER}       & \textbf{WER}      & \textbf{PER}     & \textbf{WER}       & \textbf{PER}     \\ \hline \hline
cmn                & broad         & 95.0               & 72.5             & 100                & 73.8              & 94.1                & 45.2              & 86.1                & 40.5               & 93.1              & 39.2             & \textbf{84.2}      & 36.4             \\
deu                & broad         & 74.1               & 26.0             & 76.9               & 28.0              & 72.2                & 24.5              & \textbf{63.0}       & 23.4               & 72.0              & 20.8             & 65.4               & 19.8             \\
deu                & narrow        & \textbf{50.9}      & 16.2             & 54.6               & 16.4              & 63.9                & 19.6              & 63.9                & 20                 & 57.0              & 16.6             & 59.8               & 17.9             \\
ell                & broad         & 78.9               & 32.8             & \textbf{75.4}      & 31.1              & \textbf{75.4}       & 29.2              & \textbf{75.4}       & 29.5               & 77.0              & 17.3             & 77.0               & 18.3             \\
eng us             & broad         & \textbf{66.4}      & 22.3             & 87.6               & 32.5              & 84.1                & 27                & 87.6                & 30.4               & 83.9              & 20.9             & 92.0               & 24.7             \\
eng us             & narrow        & \textbf{94.7}      & 47.5             & 97.3               & 44.5              & 97.3                & 51.8              & 97.3                & 47.9               & 97.3              & 45.1             & 97.3               & 44.1             \\
eus                & broad         & 36.8               & 4.2              & 48.3               & 5.9               & 36.8                & 4.8               & \textbf{34.5}       & 4.7                & 34.9              & 5.1              & 34.9               & 4.8              \\
fra                & broad         & 37.0               & 15.6             & 45.4               & 17.7              & \textbf{34.3}       & 14.3              & 48.1                & 19.3               & 34.6              & 11.6             & 48.6               & 15.5             \\
hin                & narrow        & 91.1               & 62.6             & 91.1               & 62.6              & 90.3                & 58.5              & 90.3                & 58.5               & \textbf{90.2}     & 29.9             & \textbf{90.2}      & 30.4             \\
hin                & broad         & 96.0               & 56.1             & 96.0               & 55.8              & 95.2                & 57.8              & 95.2                & 57.8               & \textbf{95.1}     & 47.0             & \textbf{95.1}      & 47.0             \\
ind                & broad         & \textbf{81.5}      & 30.8             & \textbf{81.5}      & 27.9              & \textbf{81.5}       & 29.7              & \textbf{81.5}       & 30.1               & 82.2              & 17.8             & 82.2               & 19.4             \\
ind                & narrow        & 85.2               & 33.0             & \textbf{82.4}      & 32.2              & \textbf{82.4}       & 31.3              & \textbf{82.4}       & 31.0               & 86.0              & 23.8             & 86.0               & 23.6             \\
kat                & broad         & 64.2               & 14.6             & 64.2               & 14.6              & \textbf{58.2}       & 11.5              & \textbf{58.2}       & 11.5               & 59.1              & 7.5              & 59.1               & 7.8              \\
kor                & narrow        & 100.0              & 52.5             & 100.0              & 52.7              & 100.0               & 52.5              & 100.0               & 51.4               & 100.0             & 100.0            & 100.0              & 100.0            \\
mya                & broad         & 97.5               & 34.4             & 97.5               & 36.5              & \textbf{95.0}       & 21.3              & \textbf{95.0}       & 23.2               & 97.4              & 29.4             & 97.4               & 35.8             \\
rus                & narrow        & 91.6               & 37.2             & 91.6               & 37.3              & \textbf{89.5}       & 36.3              & 91.6                & 37.6               & 91.5              & 35.7             & 91.5               & 37.1             \\
spa ca             & broad         & \textbf{27.8}      & 5.1              & 28.9               & 5.4               & \textbf{27.8}       & 5.1               & \textbf{27.8}       & 5.1                & 32.3              & 5.4              & 32.3               & 5.4              \\
spa ca             & narrow        & 56.7               & 17.3             & \textbf{53.6}      & 16.5              & 58.8                & 16.2              & 54.6                & 16.2               & 65.6              & 17.3             & 54.2               & 16.1             \\
tha                & broad         & 99.4               & 62.3             & 99.4               & 62.2              & 56.9                & 19.0              & 56.9                & 19.1               & 57.2              & 16.3             & \textbf{56.6}      & 16.4             \\
tur                & broad         & 53.8               & 7.8              & 63.1               & 10.1              & \textbf{46.2}       & 6.7               & 55.4                & 8.7                & 56.2              & 9.1              & 64.1               & 9.2              \\
tur                & narrow        & 76.9               & 15.6             & \textbf{66.2}      & 11.6              & 76.9                & 11.9              & 75.4                & 13.7               & 75.0              & 11.9             & 68.8               & 12.1             \\
vie                & narrow        & 100.0              & 99.8             & 100.0              & 99.8              & \textbf{79.5}       & 44.2              & \textbf{79.5}       & 44.2               & 80.2              & 36.1             & 80.2               & 36.1  \\
\hline   

\end{tabularx}
}{NWS test results} 

\end{center}  


\subsection{Results overall}
All three model types show s similar performance no matter how long they were trained. The most notable observation is that the difference between the performance of the long and the short models is rather small. For some language, the best short model performed even better than the long one or the results were the same. This was the case for narrow German (deu), narrow US English (eng us), narrow UK English (eng uk), broad Goizueta Basque (eus), broad Burmese (mya), narrow Tagalog (tgl) and narrow Turkish (tur). I find it surprising that most of these are the narrow variants as those are more detailed. I would expect a model trained for longer to catch more details. However, none of the scores were really low, the lowest being 19.4 for Basque. Additionally, the differences are never larger than 2.8 percentage points.

The feature models did not show any relevant improvement compared to the models trained without features. They did not worsen the results but performed very similarly. When the long features models are compared to the short ones, the models trained for longer reached a better performance for almost all languages. As the input data is more complex because I added the features, it might be worth exploring even longer training times. However, also the long trained trained models without features reached a better performance than the models that for trained short without features. This means that it could be possible that the feature models would still not outperform the models trained without features if both model types were trained for longer. 






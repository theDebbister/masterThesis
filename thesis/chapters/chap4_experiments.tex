
\newchap{Experiments}
\label{chap:exp}
This chapter presents the experiments and practical explorations that I conducted for this thesis. The previous chapters listed the different steps and problems that arise when trying to create and analyse a phonetic corpus. 

\section{Typewriting pdf phonetic transcriptions}
In order to make use of as much data as possible, I used a software to manually transcribe the pdf scans. The software allows to make use of neural \ac{htr} models. There exists no pre-trained IPA model but I trained my own while transcribing the documents. On the website they mention that ideally training needs 5,000 - 10,000 words already transcribed. Although my available data is not nearly enough to train a reliable model, it was a great help to transcribe. As the scans where not handwritten text, the model still reached a surprisingly good quality. For the Hebrew transcription, the model reached a \ac{wer} of 34.52\% and a \ac{cer} of 6.11\%. The two main mistakes were made for two characters that were not even in the training data. The quality of the scans differed quite a lot which had an influence on the performance of the model as well. After transcribing another document I trained the model again and transcribed the remaining documents. The transcriptions got continuously better such that in the end for the last documents I did not take me nearly as much time as in the beginning. Most of the errors resulted from characters that had not been in the previously described documents. I did not run a closer analysis so this is only my intuition.

Transkribus allows to use public models and share their own. Technically, I can share my model as well. It needs to be clarified whether it is  actually possible as there is not a lot of training data involved and the models performance differs.

\review{potentially add short table for WER and CER values for a few languages}

\section{Pronunciation dictionary coverage}
In order to get an understanding of how many words are covered in the word lists, I created a script to calculate the coverage, the \ac{wer} and the \ac{cer}. I replaced the words in the texts with the words in the word list and compared it to the reference transcription. While dealing with the full texts and the word lists, I noticed several things that are important when dealing with those texts.

\begin{itemize}
\item The pronunciation dictionaries sometimes included duplicates with different pronunciations. This is not surprising but still it needs to be handled well. A solution is to simply delete duplicate words. A close examination also showed that sometimes, the duplicate pronunciations are wrong. As it is the case with the English word ``would". \review{add this example indented}
\item For some full texts it is not clear whether their transcription is narrow or broad. On the other hand, sometimes there is no broad or narrow word list available for a specific language but only one of those. In order to find out how similar broad and narrow texts and word lists are, the calculations were run for every possible combination of each language. For languages that had both types of text and both types of word lists, the calculations were run four times. 
\item The IPA allows to transcribe intonation segments. In German, those correspond mostly to punctuation marks like end of sentence symbols or commas. But this must not be true for every case. It needs to be decided if those should be kept or potentially deleted.
\item In order to do this very simple experiment, it is necessary to tokenize the texts. This works well for languages using the Latin script. For languages like Chinese or Korean this is more difficult to accomplish. However, this issue needs to be tackled to create \ac{g2p} models anyway. I will therefore not explore this issue here.
\item WikiPron filtered some datasets for the \ac{sigm} shared task. \review{verify that}. whenever available I used the filtered version.
\end{itemize}

The results from this experiment are summarized in table \ref{tab:coverage}. Generally it is good to see, that most texts are at least partially covered by the pronunciation dictionary. A closer examination of the results shows a few language specific issues that might be relevant in further experiments.


\subparagraph{Chinese} Although the Chinese coverage is rather high, the \ac{wer} is very bad. This is due to the fact, that in the lists the tones are represented differently than in the reference text. It needs to be analysed if one of these formats can be converted into another format. \review{check the different tone transcriptions}
\subparagraph{Hebrew} The only language that is not covered at all is Hebrew. A closer examination showed that the words in the text have many diacritics, while the words in the list do not have many diacritics. Additionally, the list if very short. 

Whenever there were four experiments per language, the combination of the broad reference text written with the broad list had the best \ac{wer} and \ac{cer} or in the case of English the best \ac{wer} and a slightly worse \ac{cer}. However, this finding needs to be analysed with caution as the narrow word lists contain always less words than the broad ones. Interestingly enough, sometimes it does not matter if the text written with the broad word list is compared to the narrow transcription or the broad. In fact, for English, the text written with the broad list compared to the narrow reference shows a better \ac{cer}. This suggests that the differences in broad and narrow transcriptions are not great.
For languages where the type of the transcriptions was unclear but two lists for available, the broad list produced the better results if there was any difference. 



\begin{table}[h!]
\begin{center}
\stepcounter{mytable}
\csvautotabular{tables/stats.csv}
\caption[Coverage of Pronunciation Dictionaries]{The table shows the coverage, \ac{wer} and \ac{cer} when the pronunciation dictionaries are used to write ``The North Wind and the Sun".}
\label{tab:coverage}
\end{center}
\end{table}

\section{Automatic \ac{g2p}}
As there is only very little data available as full texts, we decided to use the short stories as test set for the experiments with \ac{g2p} models. Those texts are all manually created and are specifically created by linguists for the purpose of studying phonetics of many languages. 

\subsection*{Preprocessing}
Before any of the data can be used, it needs to be preprocessed. As I am going to use the pronunciation dictionaries which consist of single words per line, the full texts need to be prepared like that as well.
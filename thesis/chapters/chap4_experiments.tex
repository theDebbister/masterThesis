
\newchap{Experiments}
\label{chap:exp}
This chapter presents the experiments and practical explorations that I conducted for this thesis. The previous chapters listed the different steps and problems that arise when trying to create and analyse a phonetic corpus. 

\section{Typewriting pdf phonetic transcriptions}
In order to make use of as much data as possible, I used a software to manually transcribe the pdf scans. The software allows to make use of neural \ac{htr} models. There exists no pre-trained IPA model but I trained my own while transcribing the documents. On the website they mention that ideally training needs 5,000 - 10,000 words already transcribed. Although my available data is not nearly enough to train a reliable model, it was a great help to transcribe. As the scans where not handwritten text, the model still reached a surprisingly good quality. For the Hebrew transcription, the model reached a \ac{wer} of 34.52\% and a \ac{cer} of 6.11\%. The two main mistakes were made for two characters that were not even in the training data. The quality of the scans differed quite a lot which had an influence on the performance of the model as well. After transcribing another document I trained the model again and transcribed the remaining documents. The transcriptions got continuously better such that in the end for the last documents I did not take me nearly as much time as in the beginning. Most of the errors resulted from characters that had not been in the previously described documents. I did not run a closer analysis so this is only my intuition.

Transkribus allows to use public models and share their own. Technically, I can share my model as well. It needs to be clarified whether it is  actually possible as there is not a lot of training data involved and the models performance differs.

\review{potentially add short table for WER and CER values for a few languages}

\section{Pronunciation dictionary coverage}
In order to get an understanding of how many words are covered in the word lists, I created a script to calculate the coverage, the \ac{wer} and the \ac{cer}. I replaced the words in the texts with the words in the word list and compared it to the reference transcription. While dealing with the full texts and the word lists, I noticed several things that are important when dealing with those texts.

\begin{itemize}
\item The pronunciation dictionaries sometimes included duplicates with different pronunciations. This is not surprising but still it needs to be handled well. A solution is to simply delete duplicate words. A close examination also showed that sometimes, the duplicate pronunciations are wrong. As it is the case with the English word ``would". \review{add this example indented}
\item For some full texts it is not clear whether their transcription is narrow or broad. On the other hand, sometimes there is no broad or narrow word list available for a specific language but only one of those. In order to find out how similar broad and narrow texts and word lists are, the calculations were run for every possible combination of each language. For languages that had both types of text and both types of word lists, the calculations were run four times. 
\item The IPA allows to transcribe intonation segments. In German, those correspond mostly to punctuation marks like end of sentence symbols or commas. But this must not be true for every case. It needs to be decided if those should be kept or potentially deleted.
\item In order to do this very simple experiment, it is necessary to tokenize the texts. This works well for languages using the Latin script. For languages like Chinese or Korean this is more difficult to accomplish. However, this issue needs to be tackled to create \ac{g2p} models anyway. I will therefore not explore this issue here.
\item WikiPron filtered some datasets for the \ac{sigm} shared task. \review{verify that}. whenever available I used the filtered version.
\end{itemize}

The results from this experiment are summarized in table \ref{tab:coverage}. Generally it is good to see, that most texts are at least partially covered by the pronunciation dictionary. A closer examination of the results shows a few language specific issues that might be relevant in further experiments.


\subparagraph{Chinese} Although the Chinese coverage is rather high, the \ac{wer} is very bad. This is due to the fact, that in the lists the tones are represented differently than in the reference text. It needs to be analysed if one of these formats can be converted into another format. \review{check the different tone transcriptions}
\subparagraph{Hebrew} The only language that is not covered at all is Hebrew. A closer examination showed that the words in the text have many diacritics, while the words in the list do not have many diacritics. Additionally, the list is very short. 

Whenever there were four experiments per language, the combination of the broad reference text written with the broad list had the best \ac{wer} and \ac{cer} or in the case of English the best \ac{wer} and a slightly worse \ac{cer}. However, this finding needs to be analysed with caution as the narrow word lists contain always less words than the broad ones. Interestingly enough, sometimes it does not matter if the text written with the broad word list is compared to the narrow transcription or the broad. In fact, for English, the text written with the broad list compared to the narrow reference shows a better \ac{cer}. This suggests that the differences in broad and narrow transcriptions are not great.
For languages where the type of the transcriptions was unclear but two lists for available, the broad list produced the better results if there was any difference. 



\begin{table}[h!]
\begin{center}
\stepcounter{mytable}
\csvautotabular{tables/stats.csv}
\caption[Coverage of Pronunciation Dictionaries]{The table shows the coverage, \ac{wer} and \ac{cer} when the pronunciation dictionaries are used to write ``The North Wind and the Sun".}
\label{tab:coverage}
\end{center}
\end{table}

\section{Automatic \ac{g2p}}
The model that I am using for my \ac{g2p} experiments is explained in section \ref{sec:cmu}. Setting it up was not very easy as there were issues with the tensorflow version and some other dependencies. Also, they do have a pre-trained model, but this uses a completely different transcription convention than IPA. So we cannot use this model. But to test the model and have a baseline, we trained in on the data we have, to see how it performs. As there is only very little data available as full texts, we decided to use the short stories as test set for the experiments with \ac{g2p} models. Those texts are all manually created and are specifically created by linguists for the purpose of studying phonetics of many languages. 

\subsection{Training settings}
There are different settings which I will use to train the models and analyse their performance.

\review{put the high, low resource list in the appendix and explain the distinction}

\subparagraph{Setting 1: Baseline Small}
Test all languages with the CMU model where we have data available. We want to have a baseline for all languages. All models are trained separately. Broad and narrow transcriptions are treated as separate languages and thus trained separately as well. The same is true for dialects if there is any information available. American and British English are trained in different models. I used the WikiPron pronunciation dictionaries to train the model. While some of the data has been down-sampled in the shared task, I used all that was available. Whenever a filtered version used in the \ac{sigm} task was available I used that one. The model is trained with the least amount of effort. Default settings are used and no hyperparameters are changed. The model is trained for the minimum number of steps which is 10,000 in this case.  First, I trained it on those languages where I have results from the \ac{sigm} 2021 challenge. The results are compared in table \ref{tab:baseline_cmu}.

\subparagraph{Setting 2: Baseline Large}
This setting is similar to setting 1 except that the model is trained as long as possible for each language. All models have been trained for 200,000 steps and the default settings. 

\subparagraph{Setting 3: WikiPron clean}
I will train another model that is the same like the Baseline Large, but I will use the cleaned WikiPron data. What I will clean is described below (section \ref{preprocess}).

\subparagraph{Setting 4: Feature input}
The final experiment will be with the features as input.  

\subsection{Preprocessing}
\label{preprocess}
I will be working with WikiPron data and with the small \ac{nws} corpus. For my experiments I will replace the phonemes by their features from PHOIBLE. In order to make sure that this works well, I will clean the data and check that the phonemes are in PHOIBLE. In section \ref{sec:ipa} I talked about the incompleteness and difficulties of transcribing using the \ac{ipalpha}. How exactly a sound is mapped to a \ac{ipalpha} symbol also depends on whoever transcribes a particular text. The WikiPron data has been put together by many different people. That said I will carefully examine and clean the dataset. 

\subparagraph{\ac{nws} corpus}
Before any of the data can be used, it needs to be preprocessed. As I am going to use the WikiPron data which consist of single words per line with their pronunciation, the full texts need to be prepared like that as well. I did the following steps to convert the full short story texts into pronunciation dictionaries:

\begin{itemize}
\item Conversion of \textipa{\textvertline \textvertline} to \textipa{\textdoublevertline}. Some transcriptions include the double vertical line to mark a major intonation groups in the text. In some transcriptions this is a written as two single vertical lines. Those were replaced by the former to be consistent. 
\item Removal of ties bars in the phonetic transcriptions. Tie bars are not adding any valuable information. \review{get quote for this!}
\item Removal of suprasegmentals, except long and half long mark, and the extra short mark. 
\end{itemize}


\subparagraph{WikiPron} Some of the WikiPron data has already been filtered as has been explained in section \ref{data_qual}. However, as I am not only using their filtered data I will have to do some preprocessing as well and also make some additional changes also to the filtered versions. Table \ref{tab:preprocessing} shows a list of all those phonemes that are used in at least one of the WikiPron datasets for a language but are not in PHOIBLE. While it is possible that a correctly used phoneme is not in PHOIBLE, it still gives a good overview of potential ambiguities in transcription or mistakes.

\tab{tab:preprocessing}{The table shows what phonemes where changed or excluded and what the reason is for this preprocessing. All characters that were excluded are replaced by a NULL value. }{
\begin{tabularx}{\textwidth}{
| 	>{\raggedright\arraybackslash}l | 
	>{\raggedright\arraybackslash\hsize=.5\hsize\linewidth=\hsize}X | 
	>{\raggedright\arraybackslash}l  | 
	>{\raggedright\arraybackslash\hsize=1.5\hsize\linewidth=\hsize}X |}
\hline
\textbf{Phon.} & \textbf{Unicode name} & \textbf{Repl.} & \textbf{Explanation} \\
\hline
\hline
 \textipa{"} 					& \scriptsize{MODIFIER LETTER VERTICAL LINE} 				& NULL 						& \multirow[t]{6}{\hsize}{These are all \ac{ipalpha} suprasegmentals except the long and half long marker and the extra short (\textipa{: ; \u{}}). The reason why these were excluded is that they don't carry any meaning on the character level. The vertical lines, for example, mark intonation groups which only matter in a larger sentence or text context. There are a few rare occurrences of COMBINING VERTICAL LINE ABOVE which is probably meant to be MODIFIER LETTER VERTICAL LINE as they look similar. It is excluded as well.} \\
 
\textipa{""} 					& \scriptsize{MODIFIER LETTER LOW VERTICAL LINE}			& NULL						&  \\
\textipa{\textvertline} 		& \scriptsize{VERTICAL LINE} 								& NULL						&  \\
\textipa{\textdoublevertline} 	& \scriptsize{DOUBLE VERTICAL LINE} 						& NULL 						&  \\
\textipa{.} 					& \scriptsize{FULL STOP} 									& NULL 						&  \\
\textipa{\t*{}}					& \scriptsize{UNDERTIE} 									& NULL 						&  \\\hline
\textipa{\t{}}					& \scriptsize{COMBINING DOUBLE INVERTED BREVE} 				& NULL						&  \review{add explanation} \\\hline
\textipa{\t*{}}					& \scriptsize{COMBINING DOUBLE BREVE BELOW} 				& NULL						&  \review{add explanation} \\\hline
\textipa{3\textrhoticity} 		& \scriptsize{LATIN SMALL LETTER REVERSED OPEN E WITH HOOK} & \textipa{@\textrhoticity} &  \review{add explanation} \\\hline
g						 		& \scriptsize{LATIN SMALL LETTER G} 						& \textipa{g} 				&  The \ac{ipalpha} `\textipa{g}' has a different code point and is a different character than the typical keyboard small Latin `g'. This is just an \ac{ipalpha} decision. For some fonts the two characters do not look different, for some they do.  \\\hline
$\sim$							& \scriptsize{SWUNG DASH} 									& NULL 						& \multirow[t]{4}{\hsize}{All of characters make out less than 1\% of their respective dataset, most of the time it is less than 0.1\%. A close examination of the dataset and the Wiktionary transcription conventions for the respective language did not show any reason why to keep the phoneme. Note that the `v' for the tilde is only there to show character correctly.}  \\
,							 	& \scriptsize{COMMA} 										& NULL 						&  \\
\textipa{\~v} 					& \scriptsize{TILDE} 										& NULL 						& \\
\textsuperscript{\textipa{@}}	& \scriptsize{MODIFIER LETTER SMALL SCHWA} 					& NULL &  \\\hline
\textsuperscript{x}				& \scriptsize{MODIFIER LETTER SMALL X} 						& \textipa{P}				&  The \textsuperscript{x} only occurred in the broad Finnish transcription and is used to denote possible gemination. In the narrow transcriptions there is a glottal stop instead. The occurrence of glottal stops and gemination follows the same rules. Therefore, for consistency, the gemination \textsuperscript{x} is mapped to a LATIN LETTER GLOTTAL STOP.   \\
\hline
( ) 						& \scriptsize{( SUPERSCRIPT ) [ LEFT | RIGHT ] PARENTHESIS}		& NULL							& Parentheses are used to denote optionality for phonemes or tones. WikiPron actually discards those but keeps the content (\review{add GitHub reference}). I will do the same for all parenthesis found.  \\\hline
\end{tabularx}}{Preprocessing}

As the model expects the graphemes no to contain any whitespace, I replaced any whitespace in the input with underscores. This was necessary only for Vietnamese. The alternative to removing the white space would have been to splitting the grapheme at white space and align the phonemes. However, this is firstly much more work as it is not clear where to split the phonemes and it might be that the word on its own is pronounced differently. 

\href{https://en.wikipedia.org/wiki/Roundedness}{jpn} In Japanese there is the superscript letter b. This actually means compression, it is a type of rounding. This is not official, but we should actually leave it. 

\subsection{Results}


\tab{tab:baseline_cmu}{Baseline CMUSphinx results compared with \ac{sigm} 2020 and 2021 results. For each language the best score is reported no matter what year or what model. The table shows the results for setting 1. CMUSphinx provides a WER implementation which has been used to evaluate the models.}{
\begin{tabularx}{\columnwidth}{
| 	>{\raggedright\arraybackslash}l | 
	>{\raggedleft\arraybackslash}r | 
	>{\raggedleft\arraybackslash}r  | 
	>{\raggedleft\arraybackslash}X |
	>{\raggedright\arraybackslash}X |}
\hline
\textbf{ISO396-3} & \textbf{BS WER} & \textbf{BS WER \ac{nws}} & \textbf{SIG WER} & \textbf{Transcription type} \\
\hline
\hline
eng (us) & 54.40 & 87.60 & 37.43 & broad \\
fra & 7.20 & 47.20 & 5.11 & broad \\
ell & 9.80 & 83.20 & 18.67 & broad \\
kat & 0.30 & 65.20 & 0.00 & broad \\
hin & 5.60 & 87.10 & 5.11 & broad \\
jpn & 6.60 & - & 4.89 & narrow \\
kor & 28.70 & 100.00 & 16.20 & narrow \\
vie & 7.50 & 100.00 & 0.89 & narrow \\
\hline
\end{tabularx}}{BaselineSmall Results}



\tab{tab:baseline_cmu}{Baseline CMUSphinx results compared with \ac{sigm} 2020 and 2021 results. For each language the best score is reported no matter what year or what model. The table shows the results for setting 2. CMUSphinx provides a WER implementation which has been used to evaluate the models.}{
\begin{tabularx}{\columnwidth}{
| 	>{\raggedright\arraybackslash}l | 
	>{\raggedleft\arraybackslash}r | 
	>{\raggedleft\arraybackslash}r  | 
	>{\raggedleft\arraybackslash}X |
	>{\raggedright\arraybackslash}X |}
\hline
\textbf{ISO396-3} & \textbf{BS WER} & \textbf{BS WER \ac{nws}} & \textbf{SIG21 WER} & \textbf{Transcription type} \\
\hline
\hline
eng (us) & 50.70 & & 37.43 & broad \\
fra & 5.30 & & 5.11 & broad \\
ell & 7.10 & & 18.67 & broad \\
kat & 0.00 & & 0.00 & broad \\
hin & 4.40 & & 5.11 & broad \\
jpn & 6.50 & & 4.89 & narrow \\
kor & 23.40 && 16.20 & narrow \\
vie & 7.10 & & 0.89 & narrow \\
\hline
\end{tabularx}}{BaselineBig Results}


\section{Things to include and sort out later}
\subparagraph{Data Stats}
In order to get a feeling of the data and what it covers, I collected phoneme and grapheme profiles of the data and compared it to the Phoible dataset. For each language that I am working with, I have two different types of data: first the short story and second the WikiPron \ac{g2p} dictionary. For each language and each data type I got three lists:
\begin{itemize}
 \item Grapheme list: contains all graphemes in that language. Characters that need a base character like diacritics are shown together with their base character. 
 \item Phoneme list: contains all phonemes in that language. Again, diacritics and similar characters are shown with their base characters.
 \item Phoneme cluster list: Phonemes can be clustered into bigger sound groups. How to do this, is an ongoing discussion, but I used the segments library to get the clusters (compare \citet{unicode-lingu})
\end{itemize}

Having those overview for the characters for each language allowed me to compare the character vocabulary to the characters or character clusters available in the Phoible dataset. This comparison showed that quite a few characters are missing that are included in the WikiPron data and the short stories. Some problems and potential strategies to solve it:

\begin{itemize}
\item No real \ac{ipalpha}: There are sometimes characters included that are no part of the \ac{ipalpha}. There might be a reason why the authors of the transcriptions decided to use this special character to denote a particular sound, but this is not always known. A possibility is to try and map it to a character that is available in Phoible and that represents a similar sound (or even the same sound actually). 
\item Tie bars: The creators of Phoible decided to exclude tie bars because they add no real value to the transcriptions. \review{add source for this?}
\item Stress marks: Stress marks are not represented in Phoible as they do not represent a sound. 
\item Tones: Even within the \ac{ipalpha} there exist different conventions of how to represent tones. Some are better suited for different languages. Apart from different ways of representing tones, it is not always sensible to have tones represented. Mostly, tones are not written as speakers of that language know how to pronounce the tones. So, the questions is whether it is necessary to include the tones at all. When looking at the written representation it does not matter what the tones are as the basic phonemes do not change. This is of course different when the phonetic representation is mapped to a spoken representation. \review{add more on that, maybe in background chap}
\end{itemize}




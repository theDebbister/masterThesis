
\newchap{Experiments}
\label{chap:exp}
This chapter presents the experiments and practical explorations that I conducted for this thesis. The previous chapters listed the different steps and problems that arise when trying to create and analyse a phonetic corpus. 

\section{Typewriting pdf phonetic transcriptions}
In order to make use of as much data as possible, I used a software to manually transcribe the pdf scans.

The software allows to make use of neural \ac{htr} models. There exists no pre-trained IPA model but I trained my own while transcribing the documents. On the website they mention that ideally training needs 5,000 - 10,000 words already transcribed. Although my available data is not nearly enough to train a reliable model, it was a great help to transcribe. As the scans where not handwritten text, the model still reached a surprisingly good quality. For the Hebrew transcription, the model reached a \ac{wer} of 34.52\% and a \ac{cer} of 6.11\%. The two main mistakes were made for two characters that were not even in the training data. The quality of the scans differed quite a lot which had an influence on the performance of the model as well. After transcribing another document I trained the model again and transcribed the remaining documents. The transcriptions got continuously better.

\review{add more information on the model}

\section{Pronunciation dictionary coverage}
In order to get an understanding of how many words are covered in the word lists, I created a script to calculate the coverage, the \ac{wer} and the \ac{cer}. I replaced the words in the texts with the words in the word list and compared it to the reference transcription.


In order to create full texts out of pronunciation dictionaries, I created a simple python script. While dealing with the full texts and the word lists, I noticed several things that are important when dealing with those texts.

\begin{itemize}
\item The pronunciation dictionaries sometimes included duplicates with different pronunciations. This is not surprising but still it needs to be handled well. A possible solution to this is to include \ac{pos} tags. Although this is generally possible it would  mean an significantly greater effort which exceeds this thesis' scope. Another solution is to simply delete duplicate words. A close examination also showed that sometimes, the duplicate pronunciations are wrong. As it is the case with the English word "would". \review{add this example indented}
\item For some full texts it is not clear, whether their transcription is narrow or broad. On the other hand, sometimes there is no broad or narrow word list available for a specific language but only one of those. 
\item The IPA allows to transcribe intonation segments. In German, those correspond mostly to punctuation marks like end of sentence symbols or commas. But this must not be true for every case. It needs to be decided if those should be kept or potentially deleted.
\item In order to do this very simple experiment, it is necessary to tokenize the texts. This works well for languages using the Latin script. 
\item WikiPron filtered some datasets for the \ac{sigm} shared task. \review{verify that}. whenever available I used the filtered version.
\end{itemize}



\section{Automatic \ac{g2p}}
As there is only very little data available as full texts, we decided to use the short stories as test set for the experiments with \ac{g2p} models. Those texts are all manually created and are specifically created by linguists for the purpose of studying phonetics of many languages. 
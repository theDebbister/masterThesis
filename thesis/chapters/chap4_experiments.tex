
\newchap{Experiments}
\label{chap:exp}
This chapter presents the experiments and practical explorations that I conducted for this thesis. The previous chapters listed the different steps and problems that arise when trying to create and analyse a phonetic corpus. 

\section{Typewriting pdf phonetic transcriptions}
In order to make use of as much data as possible, I used a software to manually transcribe the pdf scans. The software allows to make use of neural \ac{htr} models. There exists no pre-trained IPA model but I trained my own while transcribing the documents. On the website they mention that ideally training needs 5,000 - 10,000 words already transcribed. Although my available data is not nearly enough to train a reliable model, it was a great help to transcribe. As the scans where not handwritten text, the model still reached a surprisingly good quality. For the Hebrew transcription, the model reached a \ac{wer} of 34.52\% and a \ac{cer} of 6.11\%. The two main mistakes were made for two characters that were not even in the training data. The quality of the scans differed quite a lot which had an influence on the performance of the model as well. After transcribing another document I trained the model again and transcribed the remaining documents. The transcriptions got continuously better such that in the end for the last documents I did not take me nearly as much time as in the beginning. Most of the errors resulted from characters that had not been in the previously described documents. I did not run a closer analysis so this is only my intuition.

Transkribus allows to use public models and share their own. Technically, I can share my model as well. It needs to be clarified whether it is  actually possible as there is not a lot of training data involved and the models performance differs.

\review{potentially add short table for WER and CER values for a few languages}

\section{Pronunciation dictionary coverage}
In order to get an understanding of how many words are covered in the word lists, I created a script to calculate the coverage, the \ac{wer} and the \ac{cer}. I replaced the words in the texts with the words in the word list and compared it to the reference transcription. While dealing with the full texts and the word lists, I noticed several things that are important when dealing with those texts.

\begin{itemize}
\item The pronunciation dictionaries sometimes included duplicates with different pronunciations. This is not surprising but still it needs to be handled well. A solution is to simply delete duplicate words. A close examination also showed that sometimes, the duplicate pronunciations are wrong. As it is the case with the English word ``would". \review{add this example indented}
\item For some full texts it is not clear whether their transcription is narrow or broad. On the other hand, sometimes there is no broad or narrow word list available for a specific language but only one of those. In order to find out how similar broad and narrow texts and word lists are, the calculations were run for every possible combination of each language. For languages that had both types of text and both types of word lists, the calculations were run four times. 
\item The IPA allows to transcribe intonation segments. In German, those correspond mostly to punctuation marks like end of sentence symbols or commas. But this must not be true for every case. It needs to be decided if those should be kept or potentially deleted.
\item In order to do this very simple experiment, it is necessary to tokenize the texts. This works well for languages using the Latin script. For languages like Chinese or Korean this is more difficult to accomplish. However, this issue needs to be tackled to create \ac{g2p} models anyway. I will therefore not explore this issue here.
\item WikiPron filtered some datasets for the \ac{sigm} shared task. \review{verify that}. whenever available I used the filtered version.
\end{itemize}

The results from this experiment are summarized in table \ref{tab:coverage}. Generally it is good to see, that most texts are at least partially covered by the pronunciation dictionary. A closer examination of the results shows a few language specific issues that might be relevant in further experiments.


\subparagraph{Chinese} Although the Chinese coverage is rather high, the \ac{wer} is very bad. This is due to the fact, that in the lists the tones are represented differently than in the reference text. It needs to be analysed if one of these formats can be converted into another format. \review{check the different tone transcriptions}
\subparagraph{Hebrew} The only language that is not covered at all is Hebrew. A closer examination showed that the words in the text have many diacritics, while the words in the list do not have many diacritics. Additionally, the list if very short. 

Whenever there were four experiments per language, the combination of the broad reference text written with the broad list had the best \ac{wer} and \ac{cer} or in the case of English the best \ac{wer} and a slightly worse \ac{cer}. However, this finding needs to be analysed with caution as the narrow word lists contain always less words than the broad ones. Interestingly enough, sometimes it does not matter if the text written with the broad word list is compared to the narrow transcription or the broad. In fact, for English, the text written with the broad list compared to the narrow reference shows a better \ac{cer}. This suggests that the differences in broad and narrow transcriptions are not great.
For languages where the type of the transcriptions was unclear but two lists for available, the broad list produced the better results if there was any difference. 



\begin{table}[h!]
\begin{center}
\stepcounter{mytable}
\csvautotabular{tables/stats.csv}
\caption[Coverage of Pronunciation Dictionaries]{The table shows the coverage, \ac{wer} and \ac{cer} when the pronunciation dictionaries are used to write ``The North Wind and the Sun".}
\label{tab:coverage}
\end{center}
\end{table}

\section{Automatic \ac{g2p}}
As there is only very little data available as full texts, we decided to use the short stories as test set for the experiments with \ac{g2p} models. Those texts are all manually created and are specifically created by linguists for the purpose of studying phonetics of many languages. 

\subsection{Training settings}
There are different settings which I will used to train the models.

\subparagraph{Setting 1: Baseline Small}
Test all languages with the CMU model where we have data available. We want to have a baseline for all languages. All models are trained separately. Broad and narrow transcriptions are treated as separate languages and thus trained separately as well. The same is true for dialects if there is any information available. American and British English are trained in different models as well. I used the WikiPron pronunciation dictionaries to train the model. While some of the data has been down-sampled in the shared task, I used all that was available. Whenever a filtered version was available I used that one. The model is trained with the least amount of effort. Default settings are used and no hyperparameters are changed. The model is trained for the minimum number of steps which is 10,000 in this case. 

\subparagraph{Setting 2: Baseline Large}
This setting is similar to setting 1 except that the model is trained as long as possible for each language. All models have been trained for 200,000 steps and the default settings. 

\subsection{Preprocessing}
Before any of the data can be used, it needs to be preprocessed. As I am going to use the pronunciation dictionaries which consist of single words per line, the full texts need to be prepared like that as well. I did the following steps to convert the full short story texts into pronunciation dictionaries:

\begin{itemize}
\item Conversion of \textipa{\textvertline   \textvertline} to \textipa{\textdoublevertline}. Some transcriptions include the double vertical line to mark a major intonation groups in the text. In some transcriptions this is a written as two single vertical lines. Those were replaced by the former to be consistent. 
\end{itemize}

\subsection{g2p-seq2seq CMUSphinx}
I tested a git repo where they make an already pretrained \ac{g2p} model available. Setting it up was not very easy as there were issues with the tensorflow version and some other dependencies. Also, they do have a pre-trained model, but this uses a completely different transcription convention than IPA. So we cannot use this model. But to test the model and have a baseline, we trained in on the data we have, to see how it performs. First, I trained it on those languages where I have results from the \ac{sigm} 2021 challenge. The results are compared in table \ref{tab:baseline_cmu}. 

\tab{tab:baseline_cmu}{Baseline CMUSphinx results compared with \ac{sigm} 2021 results. The table shows the results for setting 1. CMUSphinx provides a WER implementation which has been used to evaluate the models.}{
\begin{tabularx}{\columnwidth}{
| 	>{\raggedright\arraybackslash}l | 
	>{\raggedleft\arraybackslash}r | 
	>{\raggedleft\arraybackslash}r  | 
	>{\raggedleft\arraybackslash}X |
	>{\raggedright\arraybackslash}X |}
\hline
\textbf{ISO396-3} & \textbf{BS WER} & \textbf{BS WER short stories} & \textbf{SIG21 WER} & \textbf{Notes} \\
\hline
\hline
eng (us) & 54.40 & 87.60 & 37.43 & broad \\
fra & 7.20 & 57.40 & 5.11 & broad \\
ell & 9.80 & 83.20 & 18.67 & broad \\
kat & 0.30 & 65.20 & 0.00 & broad \\
hin & 5.60 & 87.10 & 5.11 & broad \\
jpn & 6.60 & - & 4.89 & narrow \\
kor & 28.70 & 100.00 & 16.20 & narrow \\
vie & 7.50 & - & 0.89 & narrow \\
\hline
\end{tabularx}}{BaselineSmall Results}



\tab{tab:baseline_cmu}{Baseline CMUSphinx results compared with \ac{sigm} 2021 results.  The table shows the results for setting 2. CMUSphinx provides a WER implementation which has been used to evaluate the models.}{
\begin{tabularx}{\columnwidth}{
| 	>{\raggedright\arraybackslash}l | 
	>{\raggedleft\arraybackslash}r | 
	>{\raggedleft\arraybackslash}r  | 
	>{\raggedleft\arraybackslash}X |
	>{\raggedright\arraybackslash}X |}
\hline
\textbf{ISO396-3} & \textbf{BS WER} & \textbf{BS WER short stories} & \textbf{SIG21 WER} & \textbf{Notes} \\
\hline
\hline
eng (us) & 50.70 & & 37.43 & broad \\
fra & 5.30 & & 5.11 & broad \\
ell & 7.10 & & 18.67 & broad \\
kat & 0.00 & & 0.00 & broad \\
hin & 4.40 & & 5.11 & broad \\
jpn & 6.50 & & 4.89 & narrow \\
kor & 23.40 && 16.20 & narrow \\
vie & 7.10 & & 0.89 & narrow \\
\hline
\end{tabularx}}{BaselineBig Results}


\section{Things to include and sort out later}
\subparagraph{Data Stats}
In order to get a feeling of the data and what it covers, I collected phoneme and grapheme profiles of the data and compared it to the Phoible dataset. For each language that I am working with, I have two different types of data: first the short story and second the WikiPron \ac{g2p} dictionary. For each language and each data type I got three lists:
\begin{itemize}
 \item Grapheme list: contains all graphemes in that language
 \item Phoneme list: contains all phonemes in that language. 
 \item Phoneme cluster list: 
\end{itemize}




\newchap{Computational Models to Create Phonetic Transcriptions}
\label{chap:tech-background}
In this chapter I present the technical background that is needed to understand how we can create phonetic transcriptions using computational models. I will first set the basis and dive into general architectures and frameworks that are commonly used and then present current state-of-the-art models. In chapter \ref{chap:ling-background}, I have pointed out that there are many different written representations of language. In the first section of this chapter, I am going to explain how a computer represents language and different scripts. 

\section{Unicode and the International Phonetic Alphabet}
\label{sec:unicode_ipa}
Computational representations of languages are very important as no matter what task we try to solve, we need to understand how written text is processed by a computer. For the present thesis, this topic is relevant for multiple reasons: 
\begin{enumerate}
\item The \ac{ipalpha} contains many special characters and many diacritics.
\item The language data is available in many different scripts.
\end{enumerate} 

It is crucial that all data files, be it phonetic or `normal' scripts, are formatted and read correctly by any machine that needs to process them. There exists a standard that is widely used to computationally represent written text. This standard is called `The Unicode Standard', in the following referred to as simply `Unicode'. 

When it comes to representing letters or signs in a machine-readable format things get very tricky, very quickly. In order to understand this fundamental problem it is necessary to understand the basic concept behind Unicode and how characters are represented behind the scenes. \citet{unicode-lingu} present an overview in their book. As discussed in chapter \ref{chap:ling-background}, there are many different kinds of what I referred to as letters or signs. From now on, I will use grapheme to denote the smallest meaningful element of any writing system. Grapheme does not imply any specific writing system nor does it get into the way of Unicode terminology. If I wish to distinguish the Unicode specifications I will use the correct Unicode term as described in this section. 

Just as a human writer must be able to uniquely identify each different grapheme, so must a computer. In Unicode, graphemes are mapped to unique numbers that can be rendered differently depending on the font and the context. There are different stages of representation until a grapheme can be represented on screen:

\begin{description}
\item[\textsc{Code point}] A unique numerical, non-negative value usually expressed as a hexadecimal number (U+0000). Allows one-to-one mapping between characters and codes. Each code point has a set of properties attributed to it. Properties like the script, uppercase or not, etc.
\item[\textsc{Character}] An abstract representation of the shape of the grapheme. Can in theory not be represented visually, as this includes a font. A Unicode character is \textit{not} the same as what we would call a grapheme in different writing systems.
\item[\textsc{Glyph}] The rendered and therefore visual representation of one or more Unicode characters that can be identified by its code point(s). A glyph is rendered in a specific font in a specific context. For me as a user, two glyphs can look completely different. Still, both glyphs might have the exact same code point and thus Unicode treats them as the exact same character. 

\begin{covsubexamples}
\label{ex:same-codepoint}
\item {\aunclfamily e} : Unicode code point U+0065
\item e : Unicode code point U+0065
\end{covsubexamples}

In example \ref{ex:same-codepoint}, both subexamples represent the same Unicode code point. Their glyphs look different because it is a different font that is used for both of them.
Sometimes one character is represented as two glyphs. It is important to note here, that the exact visual representation of a glyph is not at all defined by its code point. This means that the exact same glyph can represent more than one code points. This happens sometimes in the \ac{ipalpha}. An example is the post-alveolar click:

\begin{covsubexamples}
\label{ex:same-glyph}
\item ! : this glyph represents an exclamation mark with unicode code point U+0021.
\item \textipa{!} : this glyph represents a post-alveolar click with unicode code point U+01C3.
\end{covsubexamples}

It is striking that both glyphs in example \ref{ex:same-glyph} look exactly the same. This ambiguity becomes important when, for example, I want to count the different characters in a text.

\end{description}

Unicode code points are often organized in blocks. Blocks are not needed by Unicode but they help users to organize characters that are related into larger groups. For example: all letters of the Latin alphabet are represented by a code point. Instead of just using any random code point for all letters it makes sense to use consecutive numbers as code points. Then I can say that all basic Latin letters have a code point in the range of U+0000 to U+007F. Those blocks are helpful although not always consistent. The \ac{ipalpha} is represented in a basic block but many \ac{ipalpha} symbols are actually found in other blocks. 

Further confusion often arises from the fact, that one human-perceived grapheme is sometimes represented as more than one code point. 

\begin{description}
\item[\textsc{Grapheme clusters}] A grapheme cluster is one visual letter that is represented in Unicode as more than one code point. This is the case for diacritic marks. Diacritic marks are characters that cannot really exists without their base character. A problem with grapheme clusters is that some diacritic marks are underspecified. This means that when we want to split into clusters, we do not know if that character belongs to the left or the right base character. This is the case for many characters in the \ac{ipalpha}. Unicode does not specify these but leaves it to the user to create tailored grapheme clusters. An example is the superscript aspiration /\ipa{\super h}/. It is typically written after the respective aspirated character (/\textipa{p\super h}/), but there is no reason why it could not be written before the respective character (/\textipa{{\super h}p}/), except for convention.
\item[\textsc{Precomposed characters}] Note that sometimes, grapheme clusters can be precomposed and the combination of those two or more characters is assigned a new number. These clusters can be problematic if in a specific context, the graphemes should not be clustered but read separately. An example is the German `ä', illustrated in example \ref{ex:precomposed}.

\begin{covsubexamples}[preamble={This example shows a precomposed character and the different characters it is made of. Note the different code points. If the character was not precomposed, it would simply have two code points similar to sub-example (d).}]
\label{ex:precomposed}
\item ä : U+00E4 
\item a : U+0061
\item ¨ : U+00A8
\item \textipa{\r*e} : U+0065 U+0325
\end{covsubexamples}

\end{description} 

An additional challenge is that of picking the right font. Our standard font format can only contain about half of all the Unicode code points. It is therefore simply not possible to display the entire set of Unicode characters with one font. Many problems encountered with displaying writing systems are somehow connected to the font rather than Unicode itself \citep{unicode-lingu}.

\subparagraph{Segments library} \citet{unicode-lingu} present a Python library called segments\myfootnote{\url{https://pypi.org/project/segments/}} that can be used to process phonetic text. It includes a method to segment \ac{ipalpha} strings into grapheme clusters. Those grapheme clusters are designed in a way such that they represent sound units better. Example \ref{ex:segments} illustrates how this segmentation looks like. Different segments are separated by white space.

\begin{covexamples} 
\item \label{ex:segments} /\textipa{j u: z I \t{dZ}}/
\end{covexamples}

In the \ac{ipalpha} there exists a length marker to denote vowel lengthening. The vowel and the length marker are displayed as one segment. Characters connected by a tie bar are displayed as one segment as well. This segmentation is adapted to the actual sounds as it is not possible to pronounce a vowel and the length marker as a separated sound. However, from a Unicode perspective those are both different code points. Later when processing the data I will make use of this library.

\subsection*{Unicode normalization forms}
The above explanations make clear that there are considerable differences in what a human reader perceives and in what happens in the background. Unicode therefore provides normalization forms that can help to process written data. Unicode publishes extensive explanations along with their standards which also includes those normalization forms. I will therefore not explain everything in full detail as this is done already online \citepalias{Unicode.27.08.2021}. 

There are two important aspects to normalization. One is that we can have decomposed or composed characters. The second aspect of normalization is that we have a compatibility form and a non-compatibility form. Based on these two aspects there are four normalization forms:
\begin{description}
\item[NFD] : Canonical Decomposition
\item[NFC] : Canonical Decomposition, followed by Canonical composition
\item[NFKD] : Compatibility Decomposition
\item[NFKC] : Compatibility Decomposition, followed by Canonical Composition
\end{description}

What is important is that each normalization form results in very different behaviour if a text is processed. What the normalization form of a text is, is not necessarily visible to the human reader. All of this happens at the level of internal representation of a computer which means that it happens at the level of Unicode code points. In a decomposed string, we split the characters into their individual components. This means that characters with diacritics are split up into two or more characters. This again means that a character that is assigned one code point in a composed normalization form can in a decomposed form have more than one code points. As this is very difficult to understand on a abstract level, figures \ref{fig:norm-form1} and \ref{fig:norm-form2} show what each normalization form changes about the characters and code points.

\fig{images/UAX15-NormFig4.jpg}{fig:norm-form1}{This figure illustrates what composed and decomposed normalization forms change about the character representation. The code points are below the glyphs\myfootnote{\url{https://unicode.org/reports/tr15/}}.}{13em}{Composed and decomposed Unicode normalization}

In a composed normalization usually precomposed forms are kept. This means that some parts can be similar or the same to the decomposed version. If for a character there exists a precomposed version, the character is shown precomposed and not split up into its individual parts (see figure \ref{fig:norm-form2}, first example). 

If a normalization is according to compatibility decomposition, this means that any formatting is removed such that we receive the underlying character in its original form. Superscript characters are then shown as example \ref{ex:superscript} illustrates. 

\begin{covexamples}
\item \label{ex:superscript} \textipa{\super h} $\rightarrow$ h
\end{covexamples}

\fig{images/UAX15-NormFig6.jpg}{fig:norm-form2}{This figure illustrates the difference between all four Unicode normalization forms. The code points are below the glyphs\myfootnote{\url{https://unicode.org/reports/tr15/}}.}{22em}{Normalization forms}

How exactly these normalization forms work is not always equally important, but what is absolutely crucial is to make sure that when characters are compared or counted, the same normalization form is used. This is because when counting characters, we do not count the human perceivable glyphs, but Unicode code points.  

In Python, it is possible to convert a text that is read from a file in a normalization form into another normalization form. None of this process is visible for the human reader. Similarly, it is not possible for a human reader to see what the normalization form of a text is. This is because computational text representations depend on Unicode code points. But what the human reader perceives are the individual glyphs. And has I have pointed out, glyphs have nothing to do with the underlying Unicode representation.  

Understanding Unicode and its normalization forms is the first step to be able to create computational models for phonetic transcriptions as we need to pass text data as input to the model. In the following section, I will focus on the actual model that performs the conversion from orthographic text to phonetic text.

\section{G2P as sequence-to-sequence task}
\label{model_theory}
No matter what computational model I use, creating phonetic transcriptions can be referred to as a \acf{s2s} task. There are many other \ac{s2s} \ac{nlp} tasks whose goal it is to transform a sequence of symbols into another sequence of symbols. \ac{s2s} is not only a type of task but also an architecture for computational models that are build to solve \ac{s2s} tasks. Further down, I will introduce this architecture. Machine translation is a very well-known \ac{s2s} task \citep{Rao2015GraphemetophonemeCU}. In machine translation we transform a sequence of words in one language into another sequence of words in another language. In the present case of creating phonetic transcriptions, the input sequence is a sequence of graphemes in any script. The output sequence is a sequence of phonemes\myfootnote{Please refer to section \ref{sec:ipa} in order to understand the terminological implications of  phoneme. As it is common in research, I will stick to the term \textit{phoneme} although strictly speaking it is not always correct. Phoneme in this case just refers to any symbol that is used to represent a sound.} written in \ac{ipalpha}. This process is typically referred to as \acf{g2p} conversion. There are a few problems and characteristics of \ac{s2s} tasks that are important also for \ac{g2p} conversion:

\begin{itemize}
\item The input and output sequences are not always of the same length. It is difficult to align input and output which means to map one or more input grapheme to one or more output phoneme. Not all systems rely on aligning input and output but often it is needed to analyse the results. For example to visualize which grapheme(s) is/are mapped to which phoneme(s). 
\item Due to the open-vocabulary situation and the impossibility to cover all possible words, all systems must be able to deal with rare and unseen words \citep{Rao2015GraphemetophonemeCU, ney-joint-sequence2008}. 
\item Most of the research done in this area is limited to the English language. This is not uncommon in \ac{nlp} research. The availability of English data resources and the unavailability and struggles to find data in other languages heavily influences this research.
\end{itemize}

There exist powerful \ac{s2s} models that can deal with the first point mentioned in the listing above. Some but not all of the models I will present below can deal with the second point mentioned above while the third point does not depend on the model but on the availability of language resources. However, there are computational models that can deal better with only little available data as I will explain. 

\section{Computational models for G2P}
In the following, I am going to explain the most important model types. Sometimes, it is possible to use more than one model type in combination to exploit the advantages of each model.

\subsection{Look-up dictionary} The simplest approach to \ac{g2p} is a look-up table that allows me to store a grapheme sequence (typically a word) and a phonetic transcription of that word. If I'd like to transcribe a specific word, I can search the table for that word and get its phonetic transcription. Such a look-up-table, which is also called a dictionary, is expensive to create and has a limited coverage as it requires to add each grapheme-phoneme pair manually. Although such a system is no longer useful on its own it can still be used in addition to other, for example, statistical models \citep{ney-joint-sequence2008}.

\subsection{Rule-based models}
The first systems to create phonetic transcriptions of text were rule-based systems. Although rule-based systems are outperformed by more recent neural models \citep{Ashby-Bartley.2021, gorman-etal-2020-sigmorphon}, I will introduce them as they were an important step towards \ac{g2p} modelling. Additionally, rule-based models can be used together with other models to reach a better performance. Rule-based transcriptions models are built using linguistic pronunciation rules. For example, if in a language a certain word initial grapheme is always pronounced the same way, we can store a rule that states that if that grapheme is encountered word initially it should be transcribed using a specific phoneme. While this is a rule on grapheme level, there exist also rules on word or sub-word level \citep{mortensen-etal-2018-epitran}. In order to be able to create such a system, one needs to collect or create the pronunciation rules first which needs a lot of linguistic expertise. Another drawback of such systems is that they might fail when presented with unseen or rare words if rules are at the word level \citep{ney-joint-sequence2008}.

\subsection{N-gram models / statistical models}
N-gram models or statistical models models were used before neural models took over the field. These are sometimes referred to as traditional models. Statistical models need an alignment of graphemes and phonemes. This means that they need to map grapheme(s) to corresponding phoneme(s). This is needed because one grapheme can be realized as multiple phonemes and vice versa. It is not possible to simply have a one-to-one alignment. The necessity of grapheme-phoneme alignment is one reason why statistical models were outperformed by neural models. The main intuition about statistical models is that they, in some way or other, try to statistically model the relationship between graphemes and phonemes. A very common statistical model is the joint-sequence model. Other names for statistical models encountered in the research are \acp{fst} or \acp{wfst}.

\subsection{Neural models}
Neural \ac{g2p} models have been reported to outperform most other models \citep{Lee&Ashby.2020}. The most important of neural models will be introduced in the following. As I have pointed out earlier, \ac{g2p} modelling is a \ac{s2s} task that can be solved by \ac{s2s} models. Neural \ac{s2s} models include an encoder and a decoder or more than one of each depending on the exact implementation. The encoder first processes the entire input sequence. Once the encoder is done, the output of the encoder is passed to the decoder as input. Then the decoder processes the sequence and outputs the final sequence. Figure \ref{fig:enc-dec} shows the basic structure of the encoder-decoder architecture. What makes such a model architecture powerful is that they can map an input sequence to an output sequence of a different length and different type which is exactly what we need for \ac{g2p} conversion. As encoder and decoder we can use different types of \acp{rnn} which I present below.

\fig{images/enc-dec.jpeg}{fig:enc-dec}{This figure shows how the basic encoder-decoder or sequence-to-sequence architecture looks like. The colored RNN boxes represent the computational model that is used as the encoder and decoder. The input sequence $x$ is processed token by token by the model. The output of the encoder is passed to the decoder which outputs an output sequence $y$ again token by token. Note that the input and the ouput sequence are not of the same length\myfootnote{\url{https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346}}.}{32em}{Sequence-to-sequence architecture}

\subparagraph{RNN}
The important fact about \acp{rnn} is that they process the input sequence sequentially. In our case, this means that a \ac{rnn} performs a specific computation on each grapheme of the input sequence. It can only process the next input grapheme once it is done with the preceding grapheme. Instead of only outputting something for each grapheme, a hidden representation is passed on to be processed together with the next grapheme. This sequential processing is crucial as it means that the model has information about all graphemes preceding the grapheme that is currently processed. The output of a \ac{rnn} is a sequence of the same length as the input sequence. Each token of the output represents a phoneme. Figure \ref{fig:rnn} shows a basic \ac{rnn} model. 

A special kind of \ac{rnn} is the LSTM. LSTM means long short-term memory. As their name suggests they include what we could call a memory. Instead of just including a representation of the preceding graphemes when processing the next grapheme in the sequence, LSTMs can more flexibly decide what information is added and what information should be forgotten \citep{Olah.29.01.2022, Kostadinov.2017}. LSTMs are used a lot for \ac{g2p} conversion \citep{Lee&Ashby.2020, hammond-2021-data, gautam.2021, Rao2015GraphemetophonemeCU}.
Neural transducers are also a variation of \acp{rnn}. When neural transducers are used as encoder and decoder in a \ac{s2s} architecture they can start with the decoding step before all of the input sequence is processed by the encoder. This allows for more flexibility.

\fig{images/rnn.png}{fig:rnn}{The figure illustrates how a single RNN processes an input sequence. $x_0$ - $x_t$ denotes the input sequence. Each $x_n$ is one grapheme in our case. We can see that a representation of each segment $x_n$ is passed to the next block which processes the next segment $x_{n+1}$. $h_0$ - $h_t$ denotes the output sequence of the RNN. We can see that there is one output segment for each input segment. $A$ represents a computation of the model which could be a simple RNN but also a LSTM\myfootnote{\url{https://towardsdatascience.com/introduction-to-recurrent-neural-network-27202c3945f3}}}{30em}{RNN architecture}

\subparagraph{Transformer}
Transformers are \ac{s2s} models as well. A central concept and what makes transformers powerful models is their attention mechanism. Attention allows transformers to model dependencies between tokens of one sequence. For example: if one grapheme is pronounced differently depending on another grapheme in the sequence, the transformer can model this dependency using the attention mechanism. In addition, it can also model dependencies between the input and the output sequence. This means that while creating the output sequence, the transformer has access to the input sequence. This is possible because transformers do not have to process the input sequence sequentially like \acp{rnn} or LSTMs. They can process the entire input sequence in parallel  \citep{Alammar.03.01.2022}.


\section{Training \& evaluation}
\label{sec:train-eval}
For the training of (neural) models, it is necessary to follow a specific training regimen. The data that is used to train the model is typically split into three parts:
\begin{itemize}
    \item \textbf{Training set}: The training set is the largest part of the dataset. Typically it is about 80\% or more of the entire data. When training a model this is what we pass to the model such that it can learn how to solve the task in question.
    \item \textbf{Development or validation set}: The development or validation set consists of about 5\% - 15\% of the entire data. It is used to evaluate the model performance \textit{during} the training process. This means that we let the model predict its solutions for the data in the validation set and evaluate it by comparing it to the correct solutions. This procedure gives us an idea of how and if the model improves or not.
    \item \textbf{Test set}: The test set is often of the same size as the development set and contains about 5\% - 10\% of the data. This set is exclusively used to test the model once it is trained. This is necessary as we want to avoid that the model sees the entire data. Using a test set we can ensure that the model actually abstracted some rules from the training data and did not just learn how to reproduce the training data.
\end{itemize}

\subsection*{Evaluation metrics}
No matter what model is used, we need to appropriately evaluate it. For neural models this evaluation is performed in the test set. This means we let the model produce output phonemes for a set of input grapheme sequences and then evaluate the performance of this output. In the case of \ac{g2p} conversion we have a reference transcription and a system output that can be compared to the reference and then the difference can be quantified and given a score. The most common metric to evaluate \ac{g2p} conversion is the \ac{wer}. The lower the score, the better the model. If the \ac{wer} is 0, this means that the texts are exactly the same. The idea behind the score is that we can capture the cost that it takes to transform the system output into the reference phoneme sequence. Doing this we can find out how different the system output and the reference solution are. The following formula is used to calculate the \ac{wer}:    

\begin{equation}
\label{eq:wer}
WER = \frac{S+I+D}{N}
\end{equation} 


In equation \ref{eq:wer} $S$ stands for substitution, $I$ for insertion, $D$ for deletion and $N$ denotes the total number of words in the reference sequence. Those numbers can be calculated by using an algorithm to get the edit distance. It is quite common to multiply the resulting number with $100$ \citep{gorman-etal-2020-sigmorphon}. Note that the \ac{wer} can be more than $100$\%. This happens if, for example, the are a lot of additional insertions or deletions in the system text. If we would like to evaluate the performance of our model based on individual characters, the score is called \acf{cer}. It is calculated in the exact same way as the \ac{wer}, but instead of words everything is calculated on character basis. In the phonetic transcriptions setting, the \ac{cer} is typically replaced by the \acf{per} to match the correct terminology. The calculations are not changed. In a multilingual setting, the same model is sometimes used for different language. In such cases it is custom to use a macro-averaged \ac{wer} or \ac{cer}. This means that the sum of the scores for each language is divided by the number of languages \citep{Leung.2021}.

In the case of \ac{g2p} conversion, the \ac{wer} actually just reflects the rate of wrongly predicted words, as one sequence consists of only one word. It is therefore just 1 (or 100) minus the accuracy. The accuracy is the rate of the correctly predicted words (which means we divide all correctly predicted words by the total number of words). It is important to note, that the \ac{wer} and the \ac{per} suggest slightly different things. If each word has exactly one wrong character this means that the \ac{wer} would be 100 as all words are predicted wrongly. Now, if the words are really long, the \ac{per} score would be very good compared to the really bad \ac{wer}. If the words are rather short, the \ac{per} would get worse as well but the \ac{wer} would just stay the same. So, when analysing the results it is important to pay close attention to how those two metrics relate.


\section{State-of-the-art G2P models}
\label{section:sig}
I will now have a look at the state-of-the-art models that have been used for \ac{g2p} conversion. The \ac{sigm} \citep{Sigmorphon.2021} regularly organizes shared tasks concerned with morphology and phonology. For the years 2020 and 2021 they organized a \ac{g2p} conversion task \citep{Ashby-Bartley.2021, gorman-etal-2020-sigmorphon}. The tasks represent a first attempt at creating benchmarks for multilingual \ac{g2p} conversion. Although there is other research on \ac{g2p}, many recent publications have been made within the \ac{sigm} shared tasks. In the next sections, I will summarize the results and insights from \ac{g2p} research with a focus on those shared tasks. As they covered so many different languages, I will use their results to compare my results to. Tables \ref{tab:sota_table_long} and \ref{tab:sota} list the models and their results from those two tasks.

\subsection{Model architectures}
In section \ref{model_theory}, I explained the most important theoretical basics. Now, I familiarized myself a bit more with strategies that work well in practice and what some concrete problems are. The methodologies mentioned below are for the most part task-agnostic. This means that they often improve results on most \ac{nlp} tasks and are not specifically developed for the \ac{g2p} task.

\subparagraph{Ensembles}
Many models that are used and achieve peek performance for \ac{g2p} modelling are ensemble models. An ensemble is essentially just a pool of different models that are trained on the data with different settings or they are completely different models. The way such a model can be used for inference is that all of the models process the input and present their predicted results. Out of all possibilities, one prediction will be chosen that is then the final model output. In order to get the final output, an ensemble needs some kind of decision algorithm to output the best result. A disadvantage of ensembles is that the models need a lot of storage. Also, it is to some extent a bit of a \textit{brute-force} approach as it could lead to preferring quantity over quality. 

\subparagraph{Learning edit actions}
Instead of learning to output a phoneme sequence, a model can also learn how to edit the input sequence in order to get the output sequence. Such a model would then output an edit sequence which can be applied to the input sequence in order to obtain the final phoneme output. The model therefore learns to create sequences of edit actions. The edit actions are typically `insert', `substitute' and `delete'. The problem with this approach is that there are many possible sequences of edit actions that produce the same result. For example, it is always correct to delete every unit in the input sequence and then insert every unit from the output sequence. But this does not tell us anything about how graphemes and phonemes relate. To this end, we would also want to use substitution actions to see whether one grapheme is always substituted by the same phoneme. Imitation learning is proposed as a solution for this problem which is a type of reinforcement learning. Easily put, the idea is that the model learns to imitate the behaviour of an expert (for example, a human expert that provides correct samples of the task in question) \citep{Ai.2019}. 

\subparagraph{Multi-task learning}
What has worked well for \ac{g2p} models is to use multi-task learning. This means that the model is not only trained on one task but on multiple tasks that are related. In the present case, a model was trained on phoneme-to-grapheme conversion as well \citep{gorman-etal-2020-sigmorphon}.

\subparagraph{Neural models}
Not surprisingly, models that achieve peek performance are almost exclusively neural models \citep{gorman-etal-2020-sigmorphon}. Due to their ability to process increasingly longer sequences and the above discussed techniques like attention, they are ideal for almost all \ac{nlp} tasks. What type of neural model is chosen also depends on the amount of data available. Transformers are suggested to work better for larger datasets, while they are outperformed by LSTMs on medium-size datasets (a few thousand training pairs) \citep{gorman-etal-2020-sigmorphon}.

\subsection{Data manipulation}
A model is only as good as the data that is used to train it. While this is a very basic paradigm, in reality assuring data quality is not always easy. In this section I list a few strategies that are used to preprocess and prepare \ac{g2p} data and how to deal with too little available data. 

\subparagraph{Data quality}
\label{data_qual}
Authors mostly include a section about their preprocessing and what should be done to ensure high quality datasets. The list given below is an incomplete list of potential problems and measures taken in different settings for \ac{g2p} data \citep{Ashby-Bartley.2021}:

\begin{itemize}
\item \textbf{Exclusion of words with less than two Unicode characters or less than two phone segments}  
\item \textbf{Separation by script}: There is no obvious connection between the different scripts of a language and its pronunciation. It makes sense to treat different scripts as different languages. 
\item \textbf{Exclude foreign words with foreign pronunciations}: Foreign words in a language with their original pronunciation can add phonemes that are not in that language's phoneme inventory. If they were to be included it would make sense to include a pronunciation adapted to the actual language.
\item \textbf{Words with multiple pronunciations in word lists}: While it is possible to exclude duplicates it might also be possible to add \ac{pos} tags or other linguistic information to distinguish at least some of these words.
\item \textbf{Consistent broad transcriptions}: With broad transcriptions it is important to be consistent and not use allophones. \cite{Ashby-Bartley.2021} did this specifically for Bulgarian. They identified allophones of a language and replaced them by their respective phoneme.
\item \textbf{Linguistic variation and processes}: Some transcriptions include examples for monophthongization or deletion which are ongoing linguistic processes but should not be part of a dataset representing a standard variation. Monophthongization just means that diphthongs are replaced by monophthongs which are just single vowels. \cite{Ashby-Bartley.2021} dealt with monophthongization by choosing the longer of two transcriptions as this logically exclude the monophthonged version. This does of course only work if there is more than one pronunciation available. The idea behind this type of preprocessing is that \ac{g2p} modelling should focus on current standard variations.
\item \textbf{Tie bars}: Some languages (English and Bulgarian) have inconsistent use of tie bars. This can be corrected by replacing all inconsistencies by the tie-bar-version. It is also possible to exclude the tie bars at all.
\item \textbf{Errors in the transcriptions}: \citet{gautam.2021} noticed many errors in the WikiPron English data. They identified errors by looking at the least frequent phonemes and then check the word-pronunciation pairs where those phones occurred in. As the phoneme inventory of a language is often known, it can be used to check the phonemes in the datasets and identify uncommon ones. 
\end{itemize}

Especially the task of finding errors in the transcriptions is quite tricky. It requires a lot of knowledge about the phonology and phonetics of a specific language. 

\subparagraph{Low-resource setting}
Apart from a few well-studied examples, for most languages there is only little data available. It is therefore highly interesting and important to find solutions of how to deal with lack of data. \cite{hammond-2021-data} presented a system focusing on data augmentation methods. The primary goal of their approach was to test how successful a minimalist data augmentation model would be, knowing it would most probably not outperform any of the other models. They identified two approaches that might improve low-resource models. The first one is to use as much as possible of the development set for training. The second is to train all languages together differentiating the languages only by a tag added to the word representations. The results for these strategies were not every clear as both of these strategies were successful for some languages but did not improve the results for others.

\citet{yu-etal-2020} propose a data augmentation model for low-resource settings. The methodology applied in their approach is ensemble learning combined with a self-learning strategy. They use their ensemble to make predictions on unlabelled data. This newly created data is then added to the training data and the models are trained for another epoch on the extended data. This strategy worked well and produced good results. 

Results in a low-resource setting are still bad when only using 800 samples for training. More research needs to be done in data augmentation techniques and improving the systems to cope with only little available data.

\subparagraph{Reduce vocabulary size}
Some syllabary languages like Korean allow the decomposition into smaller units that make up the signs. Many other languages that do not use the Latin alphabet allow to be written with Latin letters. If a reduction of the character vocabulary size is possible in one of these ways, it almost always improves performance as smaller vocabulary sizes are easier to handle for models \citep{gorman-etal-2020-sigmorphon}. 


\subsection{Error and result analysis}
In this section I will list different types of analyses that have been performed by different authors on a trained model to improve future research.

\subparagraph{Broad and narrow transcriptions}
In the \ac{sigm} tasks, there are great differences in the performance of models for different languages. One possible explanation is that the datasets were a mix between broad and narrow transcriptions. As narrow transcriptions are a lot more detailed, it can be argued that this is more difficult for any system \citep{Ashby-Bartley.2021}. This assumption still needs to be analysed more closely. This differing performance for various languages calls for the question what makes a language hard to pronounce. Especially as for Georgian all models from the \ac{sigm} task reached a \ac{wer} of $0.0$. For Georgian the provided training set only contained 10,000 samples. Interestingly enough, the \ac{wer} for English which was trained on 42,000 samples reached one of the highest \acp{wer}. This suggests that English is more irregular to pronounce than Georgian. That a model trained on considerably less data for one language compared to a model trained on much more data for another language performs a lot better is a strong indication that one language is easier to pronounce.

\subparagraph{Linguistic error analysis} \citet{lo-nicolai-2021-linguistic} chose to perform an error analysis and try to minimize the frequent errors of a model in a multilingual low-resource setting. The analysis showed that often the model gets vowels and diacritics wrong. They extended the model in such a way that wrong vowel and diacritics predictions are punished more than other errors. Compared with the unchanged model, this extended model reached a better performance for some languages. The predictions with their model shows an improvement in vowel prediction. A further analysis showed that many errors still happen with vowels. Vowels get often confused with similar vowels. Their conclusion is that many of these errors make sense in a linguistic sense. 

Another type of linguistic analysis that can be performed is to analyse the data and check for uncommon pronunciations or language internal ambiguities. If a model produces a lot of wrong output because of ambiguities or uncommon data, then this is not necessarily the model's fault but just a language inherent inconsistency. As languages are generally ambiguous, this type of analysis is very insightful to find out about \textit{real} errors of the model. If a model makes a lot of mistakes although there is no underlying language inherent ambiguity this is a real mistake of the model. It should be possible for a model to abstract a transcription rule from the data if there are no ambiguities. \cite{Ashby-Bartley.2021} did such an analysis for the \ac{sigm} 2021 task which showed that many errors are due to language internal ambiguities.


\subparagraph{Include linguistic information} What has been suggested by \cite{gorman-etal-2020-sigmorphon}, is to make use of phonetic resources or rule-based systems to improve the quality of current models. The advantage of such an approach is that it is specifically tailored to the problem at hand and not at all task-agnostic. \cite{makarov-clematide-2020-cluzh} confirm this suggestion as they performed an error analysis which showed that including linguistic information such as \ac{pos}-tags might be useful. 

As is always the case with such research there are many different aspects that can be tuned in order to improve model results. For my thesis I will have a closer look at how we can use phonetic features to improve models. 
\begin{landscape}
\thispagestyle{empty}
\tab{tab:sota_table_long}{The table lists the SOTA models from the SIGMORPHON tasks in 2020 and 2021. Superscript: model numbers. Numbers in bold: best WER. Languages in bold are in the 100LC corpus. $^A$: 10,000 training samples in 2021. $^B$: 800 training samples in 2021. Model explanations: table \ref{tab:sota}.}{
\vspace{-1cm}
\begin{tabularx}{1.65\textwidth}{Xrrrrrrrrrr|rrrrr}
\hline
\textbf{ISO396-3} & \multicolumn{6}{c}{\textbf{BS20}} & \multicolumn{2}{c}{\textbf{DeepSPIN20}} & \multicolumn{2}{c|}{\textbf{IMS20}} & \textbf{BS21} & \multicolumn{1}{c}{\textbf{CL21}} & \multicolumn{2}{c}{\textbf{UBC21}} & \multicolumn{1}{c}{\textbf{DP21}}   \\

 & \multicolumn{2}{c}{\textbf{LSTM}} & \multicolumn{2}{c}{\textbf{transformer}} & \multicolumn{2}{c}{\textbf{pair n-gram}} & & & & & & CL & UBC-1 &  UBC-2 & \\

		        & WER          & PER          & WER   & PER          & WER   & PER          & WER             &PER& WER        & PER          & WER   & WER          & WER   & WER   & \\\cline{2-16}

ady$^B$         & 28.00        &\textit{6.53} & 28.44 &\textit{6.49} & 32.00 &\textit{7.56} & 24.67$^3$       & & 25.00        &\textit{5.79} &\textbf{22.00} & \textbf{22.00$^{23}$} & 25.00 & \textbf{22.00}& \\
bul$^A$ 	    & 31.11        &\textit{5.94} & 34.00 &\textit{7.89} & 41.33 &\textit{9.05} &    -            & & 22.22        &\textit{4.85} &\textbf{18.30} & 18.80$^6$    &       &       & \\
cym (wel\_sw)$^B$   &              &              &       &              &       &              &                 & &              &              &\textbf{10.00} & \textbf{10.00$^1$}    & 13.00 & 12.00 & \\
\textbf{ell (gre)$^B$}& 18.89  &\textit{3.30} & 18.89 &\textit{3.06} & 21.78 &\textit{4.05} & -               & &\textbf{18.67}&\textit{2.97} & 21.00 & 20.00$^{13}$ & 22.00 & 22.00 & \\
\textbf{eng(\_us)}&            &              &       &              &       &              &                 & &              &              & 41.94 &              &       &       & \textbf{37.43}\\
\textbf{fra (fre)$^A$}& 6.22   &\textit{1.32} & 6.89  &\textit{1.72} & 13.56 &\textit{3.12} &\textbf{5.11$^3$} && 6.89         &\textit{1.60} & 8.50  & 7.50$^{456}$ &       &       & \\
hbs$^A$         &              &              &       &              &       &              &                 & &              &              &\textbf{32.10} & 35.3$^7$    & & & \\
\textbf{hin} 	& 6.67         &\textit{1.47} & 9.56  &\textit{2.40} & 12.67 &\textit{4.05} & -               & &\textbf{5.11} &\textit{1.20} &       &              &       &       & \\
hun$^A$ 	    & 5.33         &\textit{1.18} & 5.33  &\textit{1.28} & 6.67  &\textit{1.51} &        -        & &  5.11        &\textit{1.12} & 1.80  &  \textbf{1.00$^{67}$} &       &       & \\
hye (arm\_e)$^A$   & 14.67        &\textit{3.49} & 14.22 &\textit{3.29} & 18.00 &\textit{3.90} & -               & & 12.67        &\textit{2.94} & 7.00  &  \textbf{6.40$^{7}$}  &       &       & \\
ice$^B$ 	    & 10.00        &\textit{2.36} & 10.22 &\textit{2.21} & 17.56 &\textit{3.62} &    -            & &\textbf{9.33} &\textit{2.04} & 12.00 & 10.00$^{13}$ & 13.00 & 11.00 & \\
ita$^B$         &              &              &       &              &       &              &                 & &              &              &\textbf{19.00} & 31.00$^{3}$  & 20.00 & 22.00 & \\
\textbf{jpn(\_hira)$^A$}& 7.56 &\textit{1.79} & 7.33  &\textit{1.86} & 9.56  &\textit{2.07} &\textbf{4.89$^4$} && 5.33         &\textit{1.26} & 5.20  &  5.00$^{7}$  &       &       & \\
\textbf{kat (geo)$^A$}& 26.44  &\textit{5.14} & 28.00 &\textit{5.43} & 37.78 &\textit{6.48} &       -         & & 24.89        &\textit{4.57} &\textbf{0.00}  &\textbf{0.00$^{4567}$} &       &       & \\
khm$^B$ 	    &              &              &       &              &       &              &                 & &              &              & 34.00 & 32.00$^{13}$ & 31.00 & \textbf{28.00} & \\
\textbf{kor$^A$}& 46.89        &\textit{16.78}& 43.78 &\textit{17.50}& 52.22 &\textit{15.88}& 24.00$^{13}$    & & 26.22        &\textit{4.38} & 16.30 & \textbf{16.20$^{4}$}  &       &       & \\
lav$^B$ 	    &              &              &       &              &       &              &                 & &              &              & 55.00 & \textbf{49.00$^{23}$} & 58.00 & \textbf{49.00} & \\
lit 	        &\textbf{19.11}&\textit{3.55} & 20.67 &\textit{3.65} & 23.11 &\textit{4.43} & -               & & 20.00        &\textit{3.63} &       &              &       &       & \\
mlt(\_ltn)$^B$  &              &              &       &              &       &              &                 & &              &              & 19.00 & 12.00$^{1}$  & 19.00 & 18.00 & \\
nld (dut)$^A$   & 16.44        &\textit{2.94} & 15.78 &\textit{2.89} & 23.78 &\textit{3.97} &  -              & &\textbf{13.56}&\textit{2.36} & 14.70 & 14.70$^{7}$  &       &       & \\
rum$^B$ 	    & 10.67        &\textit{2.53} & 12.00 &\textit{3.62} & 11.56 &\textit{3.55} &\textbf{9.78$^3$} && 10.22        &\textit{2.23} & 10.00 & 12.00$^{3}$  & 14.00 & 10.00 & \\
slv$^B$ 	    &              &              &       &              &       &              &                  &&              &              & 49.00 & 50.00$^{1}$  & 56.00 & \textbf{47.00} & \\
\textbf{vie$^A$}& 4.67         &\textit{1.52} & 7.56  &\textit{2.27} & 8.44  &\textit{1.79} &\textbf{0.89$^2$} &&  1.56        &\textit{0.48} & 2.50  &  2.00$^{57}$ &       &       & \\

\hline
%macro 	        & 16.84        &\textit{3.99 }& 17.51 &\textit{4.30} & 22.00 &\textit{4.92} &14.15&\textit{2.92}& 13.81        &\textit{2.76} &       &             &       &       & \\
%macro low	    &              &              &       &              &       &              &                 & &              &              & 25.10 &             & 27.10 & 24.10 & \\
%macro medium    &              &              &       &              &       &              &                 & &              &              & 10.60 &             &       &       & \\
\end{tabularx}
}{State-of-the-art models}
\thispagestyle{empty}

\end{landscape}

\thispagestyle{empty}

\tab{tab:sota}{This table presents the state-of-the-art \ac{g2p} models which will be important to compare my own results to. The results for these models can be found in table \ref{tab:sota_table_long}}{
\begin{tabularx}{\textwidth}{|X|X|p{0.6\textwidth}|}
\hline
\textbf{Model name} & \textbf{Authors} & \textbf{Model Architecture} \\
\hline
\hline
CL21  & SIG21: \citet{Clematide-Makarov.2021}      & These are seven models in total. LSTM-based neural transducer with pointer network-like monotonic hard attention trained with imitation learning. All models 1-7 are majority-vote ensembles with different number of models (5-30) and different inputs (characters or segments). \\\hline

UBC21 & SIG21: \citet{lo-nicolai-2021-linguistic}  & UBC-2 outperforms the baseline. They analysed the errors of the baseline and extend it by adding penalties for wrong vowels and wrong diacritics. Errors on vowels actually decreased. UBC-2 achieved the best macro average for low-resource languages in 2021. UBC-1 included syllable prediction which did not improve the results. \\\hline

DP21  & SIG21: \citet{gautam.2021}                 & Dialpad-1: Majority-vote ensemble consisting of three different public models (weighted FST, joint-sequence model trained with EM and a neural seq2seq), two seq2seq variants (LSTM and transformer) and two baseline variations. \\\hline

BS21 &  \citet{Ashby-Bartley.2021}                 & The baseline in 2021 is a neural transducer trained with imitation learning similar to a model submitted in 2020 \citep{makarov-clematide-2020-cluzh}. As they are so similar, I did not include the model twice, but only the newer one. \\\hline

DeepSPIN20 & SIG20: \citet{peters-martins-2020-one}& DeepSPIN-2,-3,-4: Transformer- or LSTM-based seq2seq models with sparse attention. Add language embedding to encoder or decoder states instead of language token. \\\hline

IMS20 & SIG20: \citet{yu-etal-2020}                & IMS: Self training ensemble of one n-gram-based FST and three seq2seq (vanilla with attention, hard monotonic attention with pointer, hybrid of hard monotonic attention and tagging model).\\\hline

BS20 & \citet{gorman-etal-2020-sigmorphon}         & The baseline for the task in 2020 consisted of three different model types. An LSTM, an transformer and a pair-n-gram model that is based on a weighted FST. All of them were outperformed by other models except for Lithuanian. The LSTM performed best among these three. \\\hline
\end{tabularx}
}{SOTA G2P models}

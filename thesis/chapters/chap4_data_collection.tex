\newchap{Experiments: Data Collection and Preprocessing}
\label{chap:data_collection}
The first practical part of this thesis is concerned with data collection. Although phonetics is an important subarea in linguistics, phonetic transcriptions in the form of continuous text are hard to find. If there are any transcriptions available, it is not always possible to use them as-is. In the following, I outline the different data types which are available and the different strategies that are used to convert the data into one well-formatted corpus. All the data that I collected and used for this thesis can be found on my public GitHub repository\myfootnote{\url{https://github.com/theDebbister/masterThesis}}. 

There is quite a famous set of phonetic texts which is a collection of short stories called \textit{The North Wind and the Sun}. Those stories were the starting point for my search for data. The reason for this is that the short stories are available in many different languages. As I aim at creating a multilingual phonetic corpus, the availability of data in many languages was one of the key criteria to choose the datasets. Not all languages are relevant for my thesis. In section \ref{sec:corpus}, I will present the languages that are relevant for my thesis and why those languages are relevant.

One of the first things I have noticed while collecting my datasets is the abundance of different codes and names to refer to the languages. I have decided to use the ISO 396-3\myfootnote{\url{https://iso639-3.sil.org/code_tables/639/data}} convention, not to be confused with the ISO 396-2 convention which uses sometimes completely different codes and does not cover all of my languages. ISO 396-3 distinguishes some more dialects and variants. This also means that throughout my research I paid close attention to what language variant was actually referred to. For some cases, it seems that there exists a dataset for a language which is relevant for my thesis, but actually it is not the same variant that I am looking for. To make sure I pick the same code and language, I used the Glottolog database as reference\myfootnote{\url{https://glottolog.org/glottolog}}.


\section{Transcription sources \& formats}
Phonetic transcriptions of various languages are available from different sources in different formats. From what I have found out in my research phonetic transcriptions are available as either full texts or word lists.

\subparagraph{Full Text}
For the task of \ac{g2p} conversion, phonetic transcriptions in the form of fully transcribed texts would be ideal. As became clear, it is hardly possible to find full text phonetic transcriptions. There is plenty of material describing how different languages can be transcribed but those rarely contain fully transcribed text. If they do, it is mostly limited to one or a few sentences. The only fully transcribed texts are \ac{nws} short stories which are described in section \ref{nws}.

\subparagraph{Pronunciation Dictionaries}
Another data type that is found quite often are word lists. Those are sometimes referred to as pronunciation dictionaries. However, pronunciation dictionary often means that there are words mapped to an audio representation of that word which is not what is meant in this present case. Pronunciation dictionary in this present case refers to the mapping of an orthographic word to its pronunciation using phonetic symbols. Although such lists are very handy, especially as they can easily be used to train a transcription model, transcriptions of individual words and of entire texts are not exactly the same. There are two major problems:

\begin{itemize}
\item Pronunciation depends on the context of the word in question. Word forms are ambiguous and sometimes their pronunciation differs given on their specific context. 
\item Phonetic boundaries are not always equivalent with word boundaries. Spoken language sometimes merges certain words which leads to one phonetic unit.
\end{itemize} 

Data in the form of word lists can be found quite a lot when compared to the availability of full text. An example is the Nidaba\myfootnote{\url{http://nidaba.co.uk/Contents/OriginalWordList}} website that presents a few word lists in different languages. What becomes clear when looking at this data is that there are two important limitations: First, there are only very few languages available in the Nidaba corpus that are relevant for my thesis. Second, there are different transcription conventions used.

\subparagraph{Transcription conventions}
No matter if the transcriptions are available as full text or word list, a key factor for me to choose a particular dataset is the transcription convention. The \ac{ipalpha} is a well-known convention but there is a lot of data that is available using different conventions. Although it might be possible to convert one convention into another convention, this is very tedious and often those conventions are made specifically for one language or a group of related languages and are not cross-linguistically applicable. This is why for this present thesis, I decided to only look at datasets that make use of the \ac{ipalpha} transcription convention.

\tab{tab:all_langs}{These are all languages I will use for my experiments as they are in the 100LC (section \ref{sec:corpus}) and they are available as a WikiPron word list (section \ref{sec:wikipron}). For some of those languages there is an \ac{nws} short story available as well. If not, the `Type NWS' column does not have an entry.}{
\begin{tabularx}{1.05\textwidth}{|Xll >{\raggedright}rl|}
\hline
\textbf{Iso 639-3} & \textbf{Language name}  & \textbf{Type WikiPron} & \textbf{\# Words WikiPron} & \textbf{Type NWS} \\ \hline
\hline
cmn                & Chinese                 & broad                  & 133,686                    & unk               \\
deu                & German                  & broad                  & 34,145                     & broad             \\
deu                & German                  & narrow                 & 10,984                     & narrow            \\
ell                & Greek (Modern)          & broad                  & 10,547                     & unk               \\
eng                & English US              & broad                  & 57,230                     & broad             \\
eng                & English US              & narrow                 & 1,633                      & narrow            \\
eng                & English UK              & broad                  & 60,422                     & -                 \\
eng                & English UK              & narrow                 & 1,284                      & -                 \\
eus                & Basque (Goizueta)       & broad                  & 1,742                      & broad             \\
fin                & Finnish                 & broad                  & 69,015                     & -                 \\
fin                & Finnish                 & narrow                 & 69,008                     & -                 \\
fra                & French                  & broad                  & 56,911                     & unk               \\
hin                & Hindi                   & narrow                 & 9,563                      & -                 \\
hin                & Hindi                   & broad                  & 10,812                     & unk               \\
ind                & Indonesian              & broad                  & 1,555                      & unk               \\
ind                & Indonesian              & narrow                 & 2,637                      & -                 \\
jpn                & Japanese (Hiragana)     & narrow                 & 19,689                     & -                 \\
kat                & Georgian                & broad                  & 15,123                     & broad             \\
kor                & Korean                  & narrow                 & 14,141                     & unk               \\
mya                & Burmese                 & broad                  & 4,631                      & broad             \\
rus                & Russian                 & narrow                 & 402,586                    & unk               \\
spa                & Spanish (Castilian)     & broad                  & 60,677                     & broad             \\
spa                & Spanish (Castilian)     & narrow                 & 52,190                     & narrow            \\
spa                & Spanish (Latin America) & broad                  & 48,649                     & -                 \\
spa                & Spanish (Latin America) & narrow                 & 41,845                     & -                 \\
tgl                & Tagalog                 & broad                  & 3,321                      & -                 \\
tgl                & Tagalog                 & narrow                 & 1,915                      & -                 \\
tha                & Thai                    & broad                  & 15,050                     & unk               \\
tur                & Turkish                 & broad                  & 1,789                      & unk               \\
tur                & Turkish                 & narrow                 & 1,812                      & -                 \\
vie                & Vietnamese              & narrow                 & 15,240                     & unk               \\
zul                & Zulu                    & broad                  & 1,677                      & -                 \\ \hline
\end{tabularx}}{Languages for experiments}

\section{The 100 language corpus}
\label{sec:corpus}
The basis of the data used in this thesis is a corpus provided by the SPUR lab at the \ac{uzh}. The text group of the Language and Space lab at the University of Zurich maintains a project that provides a multilingual corpus consisting of 100 language text samples \citep{UniversityofZurich.19.07.2021} which is referred to as 100LC. Those 100 languages are meant to be representative for all the world's languages. It is therefore meant to give insight on relations, similarities, differences or properties of individual languages or language families. Specifically, the goal of the team at \ac{uzh} is to use quantitative methods like statistical modelling, machine learning and information theory to study language variation and compare languages. The corpus contains 100 languages which are proposed by \citet{Comrie&Dryer.2013}. This is an online book that contains different chapters each of which shows a different linguistic feature including a map which shows the distribution of that feature over the world's languages. While the number of languages presented on the individual maps depends on the amount of research done in a specific area, the sum of all maps gives quite an impressive overview on the structure of nearly half of the world's languages. Out of the 2676 languages a sample of 100 languages was proposed. This sample does not contain too many languages from one area, neither does it contain too many languages from one family. Those are the 100 languages that are in the 100LC. Not considering the aforementioned criteria of maximizing genealogical and areal diversity can lead to misleading results in multilingual analysis. Figure \ref{fig:100lc} shows the distribution of the languages in the corpus on a world map. The different icons show the genus of the languages which is a classification of languages defined by the \ac{wals} that maintains the language description collection. The interactive map can be viewed online \citep{100LC.21.07.2021}. Table \ref{tab:100LC} in the appendix A shows all languages that are in the 100 language corpus. None of the text samples are provided by \ac{wals}. The entire corpus is provided by the SPUR team at \ac{uzh} that collected the corpus over the last few years and is continuously working on and with it.

\fig{images/100sample.png}{fig:100lc}{WALS - Map that shows the 100 Languages}{\textwidth}{100 Language Map}

\section{Data used for this thesis}
\label{sec:dataset}
Based on the outcome of my research on the availability of phonetic data, I decided to use two datasets for my experiments in this thesis. I will introduce both of them below. 

\subsection{WikiPron}
\label{sec:wikipron}
A very recent project that publishes pronunciation lists is WikiPron. The WikiPron project \citep{Lee&Ashby.2020} is an open-source Python mining tool to retrieve pronunciation data from Wiktionary. Their database contains 1.7 million word/pronunciation pairs in 165 languages. Both, the database and the mining tool, are freely available online. The WikiPron data for one language is always structured the same. It is a tsv-file that has graphemes as a first column and corresponding phonemes as the second column. While the graphemes are just listed as-is, the phonemes are split using the segments library, which I shortly presented in section \ref{sec:unicode_ipa}. The phoneme segments are separated by white spaces. In table \ref{tab:example-wikipron}, I show an example of how the WikiPron data looks like.

The WikiPron data has already been used in shared tasks which means that results on this data can easily be compared to other results. For a shared task in 2021 organized by \ac{sigm}, the WikiPron data was improved and additional scripts were added based on feedback and findings from a similar task in 2020. One major improvement was concerned with languages written in different scripts. WikiPron supports now the detection of different scripts and languages can be sorted according to those scripts. For some languages, there is a filtered version of either the broad or the narrow transcription available. Whenever WikiPron made a filtered word list available for one of my languages, I used this filtered version as a starting point.

\tab{tab:example-wikipron}{This table shows an extract from the German broad WikiPron word list. Instead of using two columns like in this example, the grapheme and the phoneme sequence are separated by a `tab' character in the original file.}{
\begin{tabular}{|l|l|}
\hline
\textbf{Grapheme}   & \textbf{Phoneme}           \\\hline\hline
a          & \textipa{P a:}              \\
aa         & \textipa{a:}                \\
aachen     & \textipa{a: x @ n}          \\
aachener   & \textipa{a: x @ n 5}        \\
aachenerin & \textipa{a: x @ n @ K I n}  \\
...        & ...               \\
übungen    & \textipa{P y: b U N @ n}    \\
übungsbuch & \textipa{y: b U N s b u: x} \\
üppig      & \textipa{Y p I \|)c}          \\
üsselig    & \textipa{Y z @ l I \|)c}      \\
œuvre      & \textipa{œ: v K @}         \\\hline
\end{tabular}
}{Example WikiPron data}

\subsection{\textit{The North Wind and the Sun}}
\label{nws}
As I have already mentioned, the \ac{nws} short stories are quite a well known corpus of phonetic transcriptions. The \ac{jpa} continuously published different phonetic transcriptions of this short story. The story is a fable said to be written by Aesop and has been translated into many languages. Additionally, for many languages there exits a phonetic transcription. Many of these texts are available already transcribed in a text file format and free to use\myfootnote{\url{https://github.com/SimonGreenhill/jipa}}. Table \ref{tab:north-wind-stories} shows the languages for which the short story is available and which are also in the 100LC. \citet{baird_evans_greenhill_2021} performed an analysis on these texts to find out how many phonetic tokens we need to cover a phonetic inventory of a language adequately. What they have found out in their study is that those short stories are by far not long enough to give a good picture of the phonetics of a language. This is of course problematic if those texts are used to demonstrate how a language works phonetically. That said, I will use these stories with care and only as an additional dataset to have more variety.

\tab{tab:north-wind-stories}{The table shows a list of all the short stories \textit{The North Wind and the
Sun} that are available as phonetic text and whose languages are in the 100
language corpus (see section \ref{sec:corpus})}{
\begin{tabular}{|llll|}
\hline
\textbf{Iso 639-3} & \textbf{Type}    & \textbf{Variation} & \textbf{Language} \\
\hline
\hline
arn      & broad and narrow &              & Mapudungun       \\
cmn      &                  & Pekinese     & Mandarin Chinese \\
deu      & broad and narrow & North German & German           \\
ell      &                  &              & Modern Greek     \\
eng      & broad and narrow & Americsn     & English          \\
eus      & broad and narrow & Goizueta     & Basque           \\
hau      & narrow           &              & Hausa            \\
heb      &                  &              & Modern Hebrew    \\
hin      & narrow           &              & Hindi            \\
ind      &                  &              & Indonesian       \\
kat      & broad and narrow &              & Georgian         \\
kor      &                  &              & Korean           \\
mya      &                  &              & Burmese          \\
pes      &                  &              & Western Farsi    \\
spa      & broad and narrow & Castilian    & Spanish          \\
tha      &                  &              & Thai             \\
tur      & broad            & Istanbul     & Turkish          \\
\hline       
\end{tabular}
}{Overview NWS stories}

Below, I include the English version of the short story including its phonetic transcription (examples \ref{ex:nws-story-ortho} and \ref{ex:nws-story-phon}).

\begin{covexamples}
\item \label{ex:nws-story-ortho} The North Wind and the Sun were disputing which was the stronger, when a traveler came along wrapped in a warm cloak. They agreed that the one who first succeeded in making the traveler take his cloak off should be considered stronger than the other. Then the North Wind blew as hard as he could, but the more he blew the more closely did the traveler fold his cloak around him; and at last the North Wind gave up the attempt. Then the Sun shined out warmly, and immediately the traveler took off his cloak. And so the North Wind was obliged to confess that the Sun was the stronger of the two.
\item \label{ex:nws-story-phon} \textipa{D@ "no{\*r}T ""wInd @n (D)@ "s2n w{@\textrhoticity} dIs"pjutIN "wItS w@z D@ "st\*rANg{@\textrhoticity}, wEn @ "r{\*r}{\ae}v@l{@\textrhoticity} ""k@m @"lAN "{\*r}{\ae}pt In @ "wo{\*r}m "klok. De @"g{\*r}id D@t D@ "w2n hu "f{@\textrhoticity}st s@k"sid@d In "mekIN D@ "t{\*r}{\ae}v@l{@\texrhoticity} "tek Iz "klok ""Af SUd bi k@n"sId{@\textrhoticity}d "st{\*r}QNg{@\textrhoticity} D@n DI "@D{@\textrhoticity}. DEnD@ "no{\*r}T ""wInd "blu @z "hA{\*r}d @z i "kUd, b@t D@ "mo{\*r} hi "blu D@ "mo{\*r} "klosli dId D@ " t{\*r}{\ae}vl{@\textrhoticity} "fold hIz "klok @"{\*r}aUnd Im; ""{\ae}n @t "l{\ae}st D@ "no{\*r}T ""wInd "" gev "2p Di @"tEmpt. "DEn D@ "s2n "SaInd ""aUt "wo{\*r}mli @nd I"midi@tli D@ t{\*r}{\ae}vl{@\textrhoticity} "tUk ""Af Iz "klok. @n "so D@ "no{\*r} ""wInd w@z @"blaIZ tI k@n"fEs D@t D@ "s2n w@z D@ "st{\*r}ANg{@\textrhoticity} @v D@ "tu.}
\end{covexamples}

\subsubsection*{Transcription of NWS stories}
A collection of the \ac{nws} stories is available in a handbook of the \ac{jpa} which is only available as a pdf scan of the original book \citepalias{JIPA2010}. Luckily, most of those texts have been transcribed and made available by Simon Greenhill\myfootnote{\url{https://github.com/SimonGreenhill/jipa}}. At least the phonetic part of it has been transcribed, the original version was still not available already transcribed. Before I knew about the availability of these texts in text file format, I did some research on \ac{ocr} for \ac{ipalpha} texts and manual transcription. While \ac{ocr} is technically possible it turns out to be very difficult for \ac{ipalpha} characters. There are tools that include \ac{ipalpha} character recognition like the ABBYY FineReader which can be acquired for a fee. The Computational Linguistics institute at the \ac{uzh} owns a version of the ABBYY tool but this version does not include the IPA module. I ran the ABBYY version without the \ac{ipalpha} module on a \ac{jpa} pdf containing said phonetic transcriptions but the result could not be used. Mostly diacritics and special phonetic symbols were not correctly transcribed. Diacritics and special phonetic symbols are exactly those graphemes that make it difficult to transcribe \ac{ipalpha} manually, so there is not point in using this tool. There are also open source tools, one of which is called tesseract\myfootnote{\url{https://github.com/tesseract-ocr/tesstrain}}. tesseract does not include the IPA alphabet. It is possible to train the model to include the IPA alphabet but this would need appropriate training data which I do not have.

As it was not possible to use \ac{ocr} to transcribe the texts, a next approach is to manually transcribe them. I experimented with a software called Transkribus\myfootnote{\url{https://readcoop.eu/de/transkribus/}} to manually transcribe the pdf scans. The software allows to make use of neural \ac{htr} models. There exists no pre-trained model for transcribing \ac{ipalpha} charachters, but I trained my own while transcribing some of the documents. On the website they mention that, ideally, training needs 5,000 - 10,000 words already transcribed. Although my available data is not nearly enough to train a reliable model (the short stories have around 40 - 100 tokens), it was a great help to transcribe. As the scans where not handwritten but machine typed text, the model still reached a surprisingly good quality. As an example: For the Hebrew transcription, the model reached a \ac{wer} of 34.52 and a \ac{cer} of 6.11. The two main mistakes were made for two characters that were not even in the training data. The quality of the scans differed quite a lot which had an influence on the performance of the model as well. After transcribing more documents I trained the model again on the newly transcribed text and let it automatically transcribed a few more documents. The transcriptions got continuously better such that in the end I did not take me nearly as much time to correct the model's automatic transcriptions as it got most of the \ac{ipalpha} characters correct. Most of the errors resulted from characters that had not been in the previously transcribed documents. 

It was interesting to see that it is relatively easy to transcribe \ac{ipalpha} text when using the right software. The orthographic texts were easier to transcribe as they do not contain as many diacritics or other special characters. Still, for some texts like Chinese or Hindi, I asked different people for help that know the respective language.

\section{PHOIBLE}
\label{sec:phoible}
A way to analyse phonetic corpora is to use phonetic features to represent each phoneme. Phonetic features are a list of properties that are overlapping with the phonetic description of each phoneme, that I have mentioned before when writing about phonetics in general in chapter \ref{chap:ling-background}. Phonetic features are a minimal list that can be used to describe unique phonemes. There exists an online database called PHOIBLE \citep{phoible} that contains over 3,000 phonemes for more than 2,000 languages. PHOIBLE includes a feature system that can describe each phoneme uniquely. In total, there are 37 phonetic features that are used to describe the phonemes. Each of the features can take on three values:
\begin{itemize}
    \item `+' (applies to this phoneme)
    \item `-' (does not apply to this phoneme)
    \item `0' (not applicable)
\end{itemize}
When we give each feature a value for each phoneme, this gives us what we can call a feature vector for each phoneme. In addition to the phonetic features, each phoneme has other features like for example what allophones are used for it in a specific language. The entire PHOIBLE inventory is structured around languages. It lists all phonemes including features for all languages that are in the database. This means that one phoneme can be listed for more than one language, but the features will always be the same for the same phoneme no matter what language. Also, this means that we cannot only use PHOIBLE to represent each phoneme as a feature vector, but we can also get an overview of the phoneme inventory of each language that is covered in PHOIBLE. Table \ref{tab:example-phonetic-features} shows an example for two feature vectors for two phonemes.

\tab{tab:example-phonetic-features}{This table shows two PHOIBLE phonetic feature vectors for the phonemes /\textipa{h}/ and /\textipa{j}/. Note that all the features would be on one line. I split them into multiple lines to show them more easily. For reasons of convenience I split some of the feature names with a white space (for example: `delayedRelease' $\rightarrow$ `delayed Release'). In the original PHOIBLE database, all feature names are written as one string.}{
\begin{tabularx}{1.1\textwidth}{|X||>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X|}
\hline
\textbf{Phoneme} & \textbf{tone}               & \textbf{stress}        & \textbf{syllabic}           & \textbf{short}               & \textbf{long}                 & \textbf{consonan-tal}            \\\hline\hline
\textipa{h}                & 0                           & -                      & -                           & -                            & -                             & -                               \\
j                & 0                           & -                      & -                           & -                            & -                             & -                               \\\hline\hline
\textbf{Phoneme} & \textbf{sonorant}           & \textbf{continuant}    & \textbf{delayed Release}     & \textbf{approximant}         & \textbf{tap}                  & \textbf{trill}                  \\\hline
\textipa{h}                & -                           & +                      & +                           & -                            & -                             & -                               \\
j                & +                           & +                      & 0                           & +                            & -                             & -                               \\\hline\hline
\textbf{Phoneme} & \textbf{nasal}              & \textbf{lateral}       & \textbf{labial}             & \textbf{round}               & \textbf{labiodental}          & \textbf{coronal}                \\\hline
\textipa{h}                & -                           & -                      & -                           & 0                            & 0                             & -                               \\
j                & -                           & -                      & -                           & 0                            & 0                             & -                               \\\hline\hline
\textbf{Phoneme} & \textbf{anterior}           & \textbf{distributed}   & \textbf{strident}           & \textbf{dorsal}              & \textbf{high}                 & \textbf{low}                    \\\hline
\textipa{h}                & 0                           & 0                      & 0                           & -                            & 0                             & 0                               \\
j                & 0                           & 0                      & 0                           & +                            & +                             & -                               \\\hline\hline
\textbf{Phoneme} & \textbf{front}              & \textbf{back}          & \textbf{tense}              & \textbf{retracted Tongue Root} & \textbf{advanced Tongue Root}   & \textbf{periodic Glottal Source}  \\\hline
\textipa{h}                & 0                           & 0                      & 0                           & 0                            & 0                             & -                               \\
j                & +                           & -                      & +                           & 0                            & 0                             & +                               \\\hline\hline
\textbf{Phoneme} & \textbf{epilaryngeal Source} & \textbf{spread Glottis} & \textbf{constricted Glottis} & \textbf{fortis}              & \textbf{raised Larynx Ejective} & \textbf{lowered Larynx Implosive} \\\hline
\textipa{h}                & -                           & +                      & -                           & -                            & -                             & -                               \\
j                & -                           & -                      & -                           & -                            & -                             & -                               \\\hline\hline
\textbf{Phoneme} & \textbf{click}              &                        &                             &                              &                               &                                 \\\hline
\textipa{h}                & -                           &                        &                             &                              &                               &                                 \\
j                & -                           &                        &                             &                              &                               &    \\\hline                            
\end{tabularx}
}{Example PHOIBLE feature vectors}

All the PHOIBLE phonetic features are listed in table \ref{tab:example-phonetic-features} together with an example. Some of them might sound familiar from the linguistic background chapter. This is because the feature system was designed based on linguistic descriptions of sounds and is supposed to be cross-linguistically applicable\myfootnote{\url{https://github.com/phoible/dev/tree/master/raw-data/FEATURES}} just as the \ac{ipalpha}. In order to understand better how those features work, I will explain them in a bit more detail. The features can be grouped into subsets of features. The features in one of the subsets are related to some extent. Some of the features and subgroups of features are more straightforward to understand, while others need more linguistic background. Although the PHOIBLE features are similar to the description of phonemes based on the \ac{ipalpha}, there are sometimes features that seem very different or incomplete. This is because on the one hand, the PHOIBLE features are designed to be minimal. This means that sometimes two or more features are used in combination to obtain an additional feature. Please see the description of `\textsc{\textbf{stress, long, short}}' below where I give an example:

\begin{description}[style=unboxed]
    \item[\textsc{stress, long, short}:] These three features are very straightforward to understand and roughly correspond to the suprasegmental chart in the \ac{ipalpha} figure \ref{fig:ipa_chart}. The special thing about the features `long' and `short' is that we can combine them to obtain an additional feature `half-long' (which is also found in figure \ref{fig:ipa_chart}). In order to do that we simply mark `long' as `+' and `short' as `+' as well. Although at the first sight this combination does not make sense as something cannot be long and short at the same time, it is a smart way to minimize the numbers of necessary features. Many other PHOIBLE features are used in a similar way to be able to represent more different phonemes by combining already existing features.
    
    \item[\textsc{tap, trill, lateral, labiodental, nasal, consonantal, labial, approximant, dorsal, coronal, anterior}:] All of these features can be inferred from the \ac{ipalpha} consonant table (see figure \ref{fig:ipa_chart}). The PHOIBLE feature `labial' is called `bilabial' in the \ac{ipalpha} consonant table. For example: The consonant /\textipa{\;B}/ has features `trill' and `labial' marked as `+' as we can infer from the \ac{ipalpha} table. The `dorsal' feature summarizes palatal, velar and uvular. The same is true for the `coronal' and `anterior' features which each represent a different subset of consonants.
    
    \item[\textsc{round, high, low, front, back}:] Those five features are inspired by the vowel schema described in section \ref{phonetics}. We can use the combination of these to describe the different vowel tongue positions. For consonants, these features are typically not applicable.
    
    \item[\textsc{syllabic, retractedTongueRoot, advancedTongueRoot, raisedLarynxEjective, loweredLarynxImplosive}:] In the \ac{ipalpha} we use a diacritic mark to represent each of these features. If a phoneme has a specific diacritic mark, we mark the respective PHOIBLE feature as `+'. Note that in the \ac{ipalpha} table, some features are named a bit differently (for example, `advanced' instead of `advancedTongueRoot').
    
    \item[\textsc{tone, click}:] For tones and clicks there exists a separate table or column in figure \ref{fig:ipa_chart}. Whenever a phoneme can be considered a click or a tone, we mark the respective feature as `+'.
    
    \item[\textsc{continuant, sonorant}:] These two features are not connected directly to the \ac{ipalpha}. They are two broad phonetic categories that are used to describe a specific manner of articulation. Both are applicable to different consonants as well as vowels.
    
    \item[\textsc{spreadGlottis, constrictedGlottis, delayedRelease, strident, distributed, tense, fortis, periodicGlottalSource, epilaryngealSource}:] These are features that have no direct connection to any of the linguistic phenomena or categories that I have introduced. But just as for all the other PHOIBLE features, they are well based on linguistic knowledge and how the sounds are produced.
\end{description}


PHOIBLE is curated very carefully and contains many different phonemes for each language. As it is based on many different studies and other phonetic databases it is very complete. This means that if there is a phoneme used in one of my datasets that cannot be found in PHOIBLE, chances are very high that it might be a mistake in my dataset. Although it is of course also possible that there is a phoneme missing in PHOIBLE. Still, comparing phonetic data to the PHOIBLE database helps to identify potential mistakes or at least very uncommon transcriptions. This is why for this present thesis, PHOIBLE serves as a phonetic reference database to clean my data. Refer to section \ref{sec:lang-profiles} where I present a possible use case of using PHOIBLE as a reference database.

\section{Pronunciation dictionary coverage}
\label{sec:coverage}
As I am using two different datasets, I want to find out how these two datasets relate. I order to do that, I am using the WikiPron lists to write the \ac{nws} stories. In a way, I used the WikiPron data as a look-up table to create phonetic transcriptions for the \ac{nws} stories. This means that for each orthographic word in the short story, I search in the WikiPron data for the respective language for that specific grapheme sequence. If the word is in the WikiPron data, I write it to a separate transcription file. If the word is not in the WikiPron data, I write a capitalized version of the orthographic word into the same transcription file. Capitalizing words that are not in the WikiPron data allows me to easily count words that occur in the short story but are not found in the WikiPron data. Examples \ref{ex:nws-story-ortho} and \ref{ex:nws-story-phon} show the orthographic version of the English \ac{nws} story and its phonetic transcription. Example \ref{ex:english-coverage-text} shows the phonetic transcription of the same English orthographic text using the WikiPron English word list as a look-up dictionary. I produced a text similar to example \ref{ex:english-coverage-text} for each language and compared it to the original \ac{nws} transcription of that language.

\begin{covexamples}
\item \label{ex:english-coverage-text} \textipa{D@ "no{\*r}T ""wInd} AND \textipa{(D)@ "s2n w{@\textrhoticity}} DISPUTING \textipa{"wItS w@z D@ "st\*rANg{@\textrhoticity}, wEn @ "r{\*r}{\ae}v@l{@\textrhoticity} ""k@m @"lAN "{\*r}{\ae}pt In @ "wo{\*r}m "klok. De @"g{\*r}id D@t D@ "w2n hu "f{@\textrhoticity}st s@k"sid@d In "mekIN D@ "t{\*r}{\ae}v@l{@\texrhoticity} "tek Iz "klok ""Af} SHOULD \textipa{bi k@n"sId{@\textrhoticity}d "st{\*r}QNg{@\textrhoticity}} THAN \textipa{DI "@D{@\textrhoticity}. DEnD@ "no{\*r}T ""wInd "blu @z "hA{\*r}d @z i "kUd, b@t D@ "mo{\*r} hi "blu D@ "mo{\*r} "klosli dId D@ " t{\*r}{\ae}vl{@\textrhoticity} "fold hIz "klok @"{\*r}aUnd Im}; AND \textipa{@t "l{\ae}st D@ "no{\*r}T ""wInd "" gev "2p Di @"tEmpt. "DEn D@ "s2n "SaInd ""aUt "wo{\*r}mli} AND \textipa{I"midi@tli D@ t{\*r}{\ae}vl{@\textrhoticity} "tUk ""Af Iz "klok.} AND \textipa{"so D@ "no{\*r} ""wInd w@z @"blaIZ tI k@n"fEs D@t D@ "s2n w@z D@ "st{\*r}ANg{@\textrhoticity} @v D@ "tu.}
\end{covexamples}

By comparing the text produced by the WikiPron data to the reference transcriptions from the \ac{jpa} articles, I could calculate the coverage, the \ac{wer} and the \ac{per} score. The coverage is calculated by simply calculating the percentage of how many words from the \ac{nws} stories are in the WikiPron data of one language. The comparison and the analysis of the metrics gives the following insights. In addition, I manually checked some errors which gave me insights about the word lists in general:

\begin{itemize}
\item For some \ac{nws} transcriptions it is not clear whether their transcription is narrow or broad. On the other hand, sometimes there is no broad or narrow word list available for a specific language but only one of those. For the short stories where the type was unclear, I tried both word list types if those are available. The analysis shows that \ac{nws} transcriptions of unknown type, where the broad list is used as a look-up table gives better results than if the narrow word list is used as a look-up table. See, for example, Indonesian (ind) or Hindi (hin) in table \ref{tab:coverage}. As this is the case, it makes sense to treat the unknown transcriptions as broad. 
\item The IPA allows to transcribe intonation segments. In German, those correspond mostly to punctuation marks like end of sentence symbols or commas. But this must not be true for every case. It needs to be decided if those should be kept or potentially deleted.
\item The pronunciation dictionaries sometimes included duplicates with different pronunciations. This is not surprising but still it needs to be handled well. A solution is to simply delete duplicate words.
\item In order to do this very simple experiment, it is necessary to tokenize the texts. This works well for languages using the Latin script. For languages like Chinese or Korean this is more difficult to accomplish. I used a special Python library called \textit{polyglot}\myfootnote{\url{https://polyglot.readthedocs.io/en/latest/}} to tokenize languages using other scripts than alphabets.
\item Interestingly enough, for some languages the coverage is really low although there are quite a lot of words in the word list. This is the case for German (narrow) where the coverage is only 22\% (see table \ref{tab:coverage}) and the word list contains more than 10,000 words. A manual analysis showed that some frequent words like `sich' and `und' are not in the word list. It becomes clear that a few thousand words are not enough to reach reasonable coverage. 10,000 - 15,000 words seem to be a lower bound for covering around 50\% of this short text.
\item For most of the languages, the \ac{per} is lower than the \ac{wer}. This is a good sign, as it suggests that if a word is in the list, it is at least partially spelled the same as in the reference text. Still, it is surprising that for broad Spanish, the \ac{wer} is actually lower than the \ac{per}. Results like this show that even if the words are covered, their phonetic transcription might be spelled differently. This is related to the fact that the \ac{ipalpha} is not really standardized. It always depends on the person who transcribes a text.
\end{itemize}

The results from this experiment are summarized in table \ref{tab:coverage}. Generally it is good to see that most texts are at least partially covered by the pronunciation dictionary. It will later be interesting to see how the neural models perform on these short texts. 

\tab{tab:coverage}{The table shows the coverage, WER and PER when the pronunciation
dictionaries are used to write \textit{The North Wind and the Sun}.}{
\begin{tabularx}{\textwidth}{|lrrrXXr|}
\hline
\textbf{Iso 639-3} &  \textbf{Coverage} &     \textbf{WER} &     \textbf{PER} & \textbf{Type ref} & \textbf{Type list} & \textbf{\# Words list} \\
\hline
\hline
cmn &     85.15 &   93.07 &   59.26 &      unk &     broad &       133,686 \\
deu &     75.00 &   72.22 &   52.67 &    broad &     broad &        34,145 \\
deu &     22.22 &   97.22 &   84.85 &   narrow &    narrow &        10,984 \\
ell &     26.32 &   84.21 &   87.74 &      unk &     broad &        10,547 \\
eng &     92.04 &   83.19 &   37.27 &    broad &     broad &        57,230 \\
eng &      7.08 &  100.00 &  108.35 &   narrow &    narrow &         1,633 \\
eus &      5.75 &   96.55 &   97.95 &    broad &     broad &         1,742 \\
ind &     22.22 &   96.30 &   88.04 &      unk &     broad &         1,555 \\
ind &      1.85 &  100.00 &  101.69 &      unk &    narrow &         2,637 \\
kat &     44.29 &   90.00 &   73.85 &    broad &     broad &        15,123 \\
mya &      7.50 &   97.50 &   97.70 &    broad &     broad &         4,631 \\
spa &     64.95 &   46.39 &   48.64 &    broad &     broad &        60,677 \\
spa &     35.05 &   98.97 &   65.92 &   narrow &    narrow &        52,190 \\
tha &     83.85 &   88.20 &   40.62 &      unk &     broad &        15,050 \\
tur &     18.46 &  100.00 &   88.89 &      unk &     broad &         1,789 \\
tur &      6.15 &  100.00 &   92.53 &      unk &    narrow &         1,812 \\
hin &     31.75 &  100.00 &   84.00 &      unk &    narrow &         9,563 \\
hin &     57.94 &   93.65 &   69.81 &      unk &     broad &        10,812 \\
kor &     18.64 &  100.00 &   51.97 &      unk &    narrow &        14,141 \\
fra &     86.11 &   51.85 &   40.76 &      unk &     broad &        56,911 \\
vie &     96.58 &   79.49 &   48.66 &      unk &    narrow &        15,240 \\
rus &     94.79 &   95.83 &   56.30 &      unk &    narrow &       402,586 \\
\hline
\end{tabularx}}{Coverage}

\section{Language profiles}
\label{sec:lang-profiles}
Once I collected my datasets, I wanted to find out what characters they include. I collected phoneme and grapheme profiles of the WikiPron data and the \ac{nws} stories and compared it to the PHOIBLE dataset. A profile lists all used characters or tokens (this depends on your chosen settings) for a text and lists its frequency. All profiles were collected for each language and each dataset separately. For each language and each dataset I got three lists:
\begin{itemize}
 \item \textbf{Grapheme list}: contains all graphemes in that language. Characters that need a base character like diacritics are shown together with their base character. 
 \item \textbf{Phoneme list}: contains all phonemes in that language. Again, diacritics are shown with their base characters.
 \item \textbf{Phoneme cluster list}: Phonemes can be clustered into bigger sound groups. How to do this is an ongoing discussion, but I used the segments library to get the clusters (compare \citet{unicode-lingu}).
\end{itemize}

Having this overview for the characters for each language allowed me to compare the character vocabulary to the characters or character clusters available in the PHOIBLE database. This comparison showed that quite a few characters are included in the WikiPron data and the short stories which cannot be found in the PHOIBLE database. My observations are listed below.:

\begin{itemize}
\item \textbf{Characters that are not included in the official \ac{ipalpha} chart}: Sometimes there are characters included in the phonetic version of the WikiPron data or the \ac{nws} short stories that are not a part of the \ac{ipalpha}. There might be a reason why the authors of the transcriptions decided to use a specific character that is not in the \ac{ipalpha} to denote a particular sound, but the reason is not always known. A possibility is to try and map it to a character that is available in PHOIBLE and that represents a similar sound (or even the same sound actually). 
\item \textbf{Tie bars}: The creators of PHOIBLE decided to exclude tie bars because they add no real value to the transcriptions\myfootnote{\url{https://phoible.github.io/conventions/}}.
\item \textbf{Stress marks and other suprasegmentals}: Stress marks are not represented in PHOIBLE as they do not represent a sound. They are included in the \ac{nws} short stories. The same is true for other suprasegmental elements.
\item \textbf{Tones}: Even within the \ac{ipalpha} there exist different conventions of how to represent tones. Some are better suited for different languages. Apart from different ways of representing tones, it is not always sensible to have tones represented at all. Mostly, tones are not written in orthographic versions as speakers of a language know how to pronounce the tones. I am going to explain more about tones in my datasets at a later stage.
\end{itemize}

I conducted this preliminary analysis at an early stage of my thesis. In chapter \ref{chap:exp} I will clean the datasets to use them for training and testing. At this point, I will give a more detailed list of what characters are in my datasets but not included in PHOIBLE and what I cleaned in the datasets.








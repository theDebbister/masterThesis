
\newchap{Introduction}
\label{chap:1_intro}
\section{Motivation}
Comparing languages and studying their similarities and differences is part of a well-established branch of traditional linguistics called comparative linguistics. 

Introduction to comparative and historical linguistics. \citep{Hock&Joseph.2019}

The analysis of large amounts of text in any language is commonly referred to as corpus linguistics. Corpus linguistics allows for both qualitative and quantitative analysis of text. Although text can refer to written or spoken language, most corpora contain written text \citep{McEnery&Hardie.2011}. Multilingual corpora can be used to compare languages.
\review{Add quick intro into corpus linguistics, quantitative analysis, this is essentially what is done with the corpus. \citep{McEnery&Hardie.2011}}

The text group of the Language and Space lab at the University of Zurich maintains a project that provides a multilingual corpus consisting of 100 language text samples \citep{UniversityofZurich.19.07.2021}. Those 100 languages are meant to be representative for all the world's languages which is explained in more detail in section \ref{corpus}. It is therefore meant to give insight on relations, similarities, differences or properties of individual languages or language families. Specifically, their goal is to use quantitative methods like statistical modelling, machine learning and information theory to study language variation and compare languages. While there are many different types of analyses that can be performed on those text samples, the question keeps coming up if analyses of speech versions of those languages might not be more accurate or give better insight. Although this is not sure and simply a suggestions, it needs to be proven that working on text versions only represents languages well enough to present generalizable results. This present thesis ties in with this open research question. The goal is to collect phonetic transcriptions of the corpus. The same analyses can be performed on the phonetic texts which can be compared to the text analysis. In order to add a phonetic corpus to the already existing one, various steps need to be performed which are outlined in section \ref{outline}.

The collection a corpus of phonetic transcriptions can be useful for different tasks other than for this current thesis. Phonetic transcriptions of written text can be an important step in the process of speech technologies (like speech synthesis). \review{add more on usage of phonetic transcriptions}


\section{Goals \& methods}
The primary goal of this thesis is to create phonetic transcriptions of as many languages as possible which are in the already existing corpus. Given the explanations above, the steps I aim to conclude to reach this goal are the following:
\begin{enumerate}
 \item Data collection: The given dataset contains no phonetic transcriptions of those 100 languages. The first step is to find already existing data. 
 \item Phonetic transcriptions: As existing data will not be available in sufficient amounts to perform meaningful analysis, the next step is to actually create phonetic transcriptions of as many languages as possible of the corpus. 
 \item Calculations and Analysis: Once the transcriptions have been obtained, the newly created phonetic corpus can be analysed and calculations can be performed.
 \item Based on the steps before, I will answer the following final question: Is it essential for the study of multilingual corpora to perform analyses on phonetic text (i.e. speech representations) rather than only written text? \review{depends on what can be done before...}
\end{enumerate}

\section{Research questions}
\label{outline}


\section{Thesis structure}

The thesis is subdivided into \review{six} chapters including a final conclusion. Chapter \ref{chap:1b_background} sets the boundaries of the theoretical background. It presents the linguistic foundation of phonetics and phonology, an introduction to corpus linguistics or rather corpus phonetics and finally an overview of the possibilities for automated creation of phonetic transcriptions. Chapter \ref{chap:2_data} introduces to the struggle of data collection. It explains the various data types and how those can be used. Chapter \ref{chap:3_model} dives deeper into the possibilities for creating phonetic transcriptions and what models can be used to create those. Chapter \ref{chap:4_exp} presents my own experiments to create phonetic transcriptions of the corpus.


\newchap{Research Background}
\label{chap:1b_background}

\section{The corpus}
\label{corpus}
The corpus contains 100 languages which are proposed by \citet{Comrie&Dryer.2013}. This online book contains different chapters each of which shows a different linguistic feature including a map which shows the distribution of that feature over the world's languages. While the number of languages presented on the individual maps depends on the amount of research done in a specific area, the sum of all maps gives quite an impressive overview on the structure of nearly half of the world's languages. Out of the 2676 languages a sample of 100 languages was chosen. This sample does not contain too many languages from one area, neither does it contain too many languages from one family. Not considering the aforementioned criteria of maximizing genealogical and areal diversity can lead to misleading results. Figure \ref{fig:100lc} shows the distribution of the corpus on a world map. The different icons show the genus of the languages which is a classification of languages defined by the \ac{wals} team that maintains the language collection. The interactive map can be viewed online \citep{100LC.21.07.2021}. Table \ref{tab:100LC} in the appendix A shows all languages that are in the 100 language corpus. 

%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
\fig{images/100sample.png}{fig:100lc}{WALS - 100 Language Sample}{\textwidth}{100 Language Sample}

\section{Corpus linguistics and quantitative analysis}
The relation between spoken and written language. Remember that writing systems came only much later compared to language in general. Can they capture language as such well enough? Computational linguistics deals mostly with written languages, what does linguistics say and do?

\section{Introduction to phonetics and phonology}
Given that phonetics and phonology is a sub-area of traditional linguistics and often only touched on superficially in computational linguistics, I will summarise the most important assumptions and terms concerning said field. A very important terminological distinction is between phonetics and phonology. While phonetics refers to the study of actual sounds, phonology refers to the study of sound \textit{systems}. In phonetics, it is not so much important what the different sounds mean, but how they are produced and perceived and what different sounds a human being can produce and perceive at all. When it comes to human communication using spoken language, many of these sounds are not actually used to produce distinguishable meaning. This is why on the other hand phonology is important to describe the set of distinguishable sounds that make up a language. For example: the letter /r/ in English can be pronounced in many different ways. None of those pronunciations produces a change in meaning. This means that there exist many different \textit{phonetic} sounds but only one \textit{phonological}. Those sounds are referred to as phone and phoneme respectively. While there are infinitely many phones there are only finitely many phonemes in a language. Not all different possible sounds are actually considered qualitatively `good' sounds of a language. Usually there is a subset of all possible phones that is accepted as `good quality sounds' within all different dialects of a language \citep{Intro.2007}. An obvious example being loudness: Although very silent speech produces correct phones, these are not 'good quality' as they simply cannot be understood. Or speaking in English with hardly any mouth and tongue movement. Although this produces understandable sound, it is not generally considered good speech. 

The alphabets used to represent sounds in different languages do not uniquely map a letter to one specific phoneme. Most of the time, there is a standard pronunciation of each letter that is trained by reciting the alphabet. However, in reciting the alphabet there is a vowel added to the consonants in order to pronounce them more easily. These explanations make clear that the mapping of written text to spoken text in various languages is complex. In order to make things easier, there is the International Phonetic Alphabet (IPA) that can be used to transcribe any text in any language to a phonetic text \citep{Intro.2007}. 

It is important to note at this point that the terms phonetic and phonemic respectively phone and phoneme are sometimes used interchangeably. Their linguistic definition is clear while the definition on the computational side is often less strict. \review{definition of phoneme / phone, i.e. phonemic, phonic, correct linguistic one
and then the one that is used e.g. in Lee et al. [2020], foot note 4}

\subsection*{Writing systems}
Unlike spoken language that was a part of human interaction all the time, writing systems only developed over time. A single grapheme can represent a sound ... (\href{https://www.youtube.com/watch?v=-sUUWyo4RZQ&list=PL8dPuuaLjXtP5mp25nStsuDzk2blncJDW&index=17}{video})



\section{Corpus phonetics}
Due to recent technological advancement it has become possible to store large digital collections of speech recordings and their aligned transcriptions. These possibilities gave rise to a wider acknowledgement of corpus phonetics. Corpus phonetics deals with an abundance of linguistic variation. In addition to language, style or vocabulary variation, there are differences in dialect and idiolect, physiological state of the speakers and their attitude \citep{Liberman.2019, Chodroff.19.07.2019}. Many methods and tools used in corpus phonetics are based on \ac{asr} algorithms or simple programming \citep{Chodroff.19.07.2019}.

\section{Automated phonetic transcription}
Creating phonetic transcriptions is essentially a \ac{s2s} task. Like other \ac{nlp} tasks its goal is to transform a sequence of characters into another sequence of characters.  

\subsection{Rule-based models}


\subsection{N-gram Models / Statistical models}
These are sometimes referred to as traditional models. 


\subsection{Neural models}
Neural G2P models have been reported to outperform most other models \citep{Lee&Ashby.2020}. Many researchers experiment with different variants of LSTM models \citep{Lee&Ashby.2020, hammond-2021-data, gautam.2021}. 

transducers: those are like automaton. Unlike automaton that only tell you if a certain sequence is in a particular language, transducers output something at every state. 

seq2seq: condition output sequence on entire input sequence. This does not work well for input that gets continuously longer or very long input sequences. 

Neural transducers, as presented by \citet{jaitly2016neural}, extend previously used \ac{s2s} models. They can treat more arriving input without having to redo the entire calculation for the entire updated sequence. At each time step, the neural transducer can output zero to many output symbols. 


A problem with creating phonetic transcriptions is that the input and output segments are not always of the same length. It is difficult to align input and output. 


Generally, there is a difference between models that assume conditional independence between the each output step (e.g. Hidden Markov Models) and there are models that do not make this assumption but condition the current output on the entire sequence before (seq2seq). Seq2seq models, however, have to wait until the full input sequence is processed before they can start decoding.  
